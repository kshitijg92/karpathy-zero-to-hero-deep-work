{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705158f5",
   "metadata": {},
   "source": [
    "In this notebook, we will be working through [Building makemore Part 2: Training Diagnostics](https://www.youtube.com/watch?v=P6sfmUTpUmc) by Andrej Karpathy. This is the fourth video in the \"Neural Networks: Zero to Hero\" series and covers part 2 of it.\n",
    "\n",
    "In this video we will look at diagnostic tools that we can use to see if our models are training optimally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e0a53",
   "metadata": {},
   "source": [
    "# Section 1: \"Pytorchify\" Our Model\n",
    "\n",
    "In the previous notebook, we had a very haphazard way of creating the model. Lets first package those into layers similar to how pytorch does it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8253b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75d359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 names loaded\n",
      "Examples: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "Vocabulary size: 27\n",
      "Mappings: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# lets copy over some stuff from the previous notebook\n",
    "\n",
    "# utility to read dataset\n",
    "DATASET_PATH = '../names.txt'\n",
    "SPECIAL_TOKEN = \".\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_dataset():\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        rows = [row.strip() for row in f.readlines()]\n",
    "    return rows\n",
    "\n",
    "# Load dataset\n",
    "words = get_dataset()\n",
    "print(f\"{len(words)} names loaded\")\n",
    "print(f\"Examples: {words[:8]}\")\n",
    "\n",
    "# Build character mappings — identical to lesson 2\n",
    "# '.' is our special start/end token at index 0, then a=1, b=2, ..., z=26\n",
    "all_characters = [SPECIAL_TOKEN] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(all_characters)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Mappings: {itos}\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81272af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   182,625 examples from 25,626 names\n",
      "Validation:  22,655 examples from 3,203 names\n",
      "Test:        22,866 examples from 3,204 names\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words, block_size=3):\n",
    "    \"\"\"\n",
    "    Convert a list of words into (X, Y) tensors for training.\n",
    "\n",
    "    X shape: (N, block_size) — each row is a context window of character indices\n",
    "    Y shape: (N,) — each element is the target character index\n",
    "\n",
    "    This function will be called three times: once each for train, val, and test splits.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + SPECIAL_TOKEN:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 80/10/10 split at the word level\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size=3)     # training\n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size=3)  # validation (\"dev\" set)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size=3)      # test\n",
    "\n",
    "Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "Xdev, Ydev = Xdev.to(device), Ydev.to(device)\n",
    "Xte, Yte = Xte.to(device), Yte.to(device)\n",
    "\n",
    "print(f\"Training:   {Xtr.shape[0]:>7,} examples from {n1:,} names\")\n",
    "print(f\"Validation: {Xdev.shape[0]:>7,} examples from {n2-n1:,} names\")\n",
    "print(f\"Test:       {Xte.shape[0]:>7,} examples from {len(words)-n2:,} names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbd09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is the code we had at the end of the previous notebook\n",
    "\n",
    "def get_params(embed_dim, block_size, n_hidden):\n",
    "\n",
    "    tanh_gain = 5./3  # correction factor for tanh squashing variance\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C  = torch.randn((vocab_size, embed_dim),            generator=g)\n",
    "    W1 = torch.randn((embed_dim * block_size, n_hidden), generator=g) * (tanh_gain / (embed_dim * block_size) ** 0.5)  # Kaiming with tanh gain\n",
    "    b1 = torch.randn(n_hidden,                        generator=g) * 0.01  # small, near zero\n",
    "    W2 = torch.randn((n_hidden, vocab_size),           generator=g) * (1 / (n_hidden) ** 0.5)  # Xavier (gain=1, no activation)\n",
    "    b2 = torch.randn(vocab_size,                       generator=g) * 0.01  # small, near zero\n",
    "\n",
    "    bngain = torch.ones((1, n_hidden))\n",
    "    bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "    # add tracking variables which are NOT part of the training and thus dont receive gradients\n",
    "    bnstd_running = torch.ones((1, n_hidden))\n",
    "    bnmean_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "    parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return bnstd_running, bnmean_running, parameters\n",
    "\n",
    "def train_model(params, n_steps=200000, batch_size=32):\n",
    "\n",
    "    bnstd_running, bnmean_running, parameters = params\n",
    "    C, W1, b1, W2, b2, bngain, bnbias = parameters\n",
    "\n",
    "    stepi = []\n",
    "    lossi = []\n",
    "    loglossi = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for i in pbar:\n",
    "        # Mini-batch: randomly sample 32 examples\n",
    "        ixds = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n",
    "\n",
    "        # Forward pass\n",
    "        emb = C[mini_batch_inp]                                        # (32, 3, 10)\n",
    "        emb_cat = emb.view(emb.shape[0], -1)                            # (32, 30)\n",
    "        hidden_layer_preactivation = emb_cat @ W1 + b1                  # (32, 200)\n",
    "\n",
    "        # CONVERT hidden_layer_preactivation to unit gaussian\n",
    "        _mean = hidden_layer_preactivation.mean(axis=0, keepdim=True) # (1, 200) take mean across the samples in the mini batch\n",
    "        _std = hidden_layer_preactivation.std(axis=0, keepdim=True) # (1, 200) take std across the samples in the mini batch\n",
    "\n",
    "        hidden_layer_preactivation = (hidden_layer_preactivation - _mean) / _std # convert to unit gaussian\n",
    "\n",
    "        # move the running average slightly based on the currnet mean and std directions\n",
    "        with torch.no_grad():\n",
    "            bnmean_running = 0.999 * bnmean_running + 0.001 * _mean\n",
    "            bnstd_running = 0.999 * bnstd_running + 0.001 * _std\n",
    "\n",
    "        # scale and shift\n",
    "        hidden_layer_preactivation = hidden_layer_preactivation*bngain + bnbias\n",
    "\n",
    "        h = torch.tanh(hidden_layer_preactivation)  # (32, 200)\n",
    "        logits = h @ W2 + b2                                           # (32, 27)\n",
    "        loss = F.cross_entropy(logits, mini_batch_target)\n",
    "\n",
    "        # Backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Learning rate step decay: 0.1 for first 100K steps, then 0.01\n",
    "        lr = 0.1 if i < 100000 else 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "        # Track loss\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.item())\n",
    "        loglossi.append(loss.log10().item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.data:.4f}\")\n",
    "\n",
    "    return stepi, lossi, loglossi, hidden_layer_preactivation, h, logits, bnmean_running, bnstd_running\n",
    "\n",
    "# now we can use these running mean and std during eval\n",
    "@torch.no_grad()\n",
    "def eval_loss(X, Y, params, embed_dim, block_size):\n",
    "\n",
    "    bnstd_running, bnmean_running, parameters = params\n",
    "    C, W1, b1, W2, b2, bngain, bnbias = parameters\n",
    "\n",
    "    emb = C[X]\n",
    "    emb_cat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    hidden_layer_preactivation = emb_cat @ W1 + b1\n",
    "    hidden_layer_preactivation = (hidden_layer_preactivation - bnmean_running) / bnstd_running\n",
    "    hidden_layer_preactivation = hidden_layer_preactivation*bngain + bnbias\n",
    "\n",
    "    h = torch.tanh(hidden_layer_preactivation)  # (32, 200)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()\n",
    "\n",
    "# bnstd_running, bnmean_running, parameters = get_params(embed_dim, block_size, n_hidden)\n",
    "\n",
    "# stepi, lossi, loglossi, hidden_layer_preactivation, h, logits, bnmean_running, bnstd_running = \\\n",
    "#     train_model([bnstd_running, bnmean_running, parameters])\n",
    "\n",
    "# print(f\"Final Training loss: {eval_loss(Xtr, Ytr,[bnstd_running, bnmean_running, parameters], embed_dim, block_size)}, \\\n",
    "#     Validation Loss: {eval_loss(Xdev, Ydev, [bnstd_running, bnmean_running, parameters], embed_dim, block_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8385e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Layer\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features, bias=True, device='cpu'):\n",
    "\n",
    "        initialization_factor = num_input_features ** 0.5 # kaiming he initialization\n",
    "\n",
    "        self.weights = \\\n",
    "            torch.randn((num_input_features, num_output_features), generator=g, device=device) / initialization_factor\n",
    "        self.bias = torch.zeros(num_output_features, device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights # note that we keep the output under self so that we can access it later for diagnostics\n",
    "        if self.bias:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias] if self.bias else [self.weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fd0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batchnorm layer\n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, num_input_features, momentum=0.1, eps=1e-5, device='cpu'):\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # learnable scale and shift parameters (gain and bias)\n",
    "        self.gamma = torch.ones(num_input_features, device=device)\n",
    "        self.beta = torch.zeros(num_input_features, device=device)\n",
    "\n",
    "        # Exponential moving average (EMA) tracking\n",
    "        self.running_mean = torch.zeros(num_input_features, device=device)\n",
    "        self.runnins_vars = torch.ones(num_input_features, device=device)\n",
    "\n",
    "        self.training = True # batchnorm behaves different during training vs during inference/eval\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "\n",
    "        if self.training:\n",
    "            xmean = x.mean(axis=0, keepdim=True) # mean across batch\n",
    "            xvars = x.var(axis=0, keepdim=True) # variance across batch\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvars = self.runnins_vars\n",
    "\n",
    "        # normalize\n",
    "        self.out = (x - xmean) / torch.sqrt(xvars + self.eps)\n",
    "\n",
    "        # scale and shift\n",
    "        self.out = self.gamma * self.out + self.beta\n",
    "\n",
    "        # EMA update\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad(): # these are not learnable params\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + xmean * self.momentum\n",
    "                self.running_vars = self.running_vars * (1 - self.momentum) + xvars * self.momentum\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c26493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fn to create a stack of layers\n",
    "\n",
    "def setup_model(embed_dim,\n",
    "                block_size,\n",
    "                n_hidden_per_layer,\n",
    "                num_hidden_layers = 4,\n",
    "                bias = True,\n",
    "                tanh = True,\n",
    "                batchnorm = True,\n",
    "                output_gain = 0.1,\n",
    "                tanh_gain = 5./3,\n",
    "                device = 'cpu'\n",
    "                ):\n",
    "\n",
    "    # create embedding lookup\n",
    "\n",
    "    C = torch.randn((vocab_size, embed_dim), generator=g, device=device)\n",
    "\n",
    "    flattened_input_features = embed_dim * block_size\n",
    "\n",
    "    layers = []\n",
    "    hidden_layers = []\n",
    "    # setup input\n",
    "\n",
    "    input_layer = Linear(flattened_input_features, n_hidden_per_layer, bias=bias, device=device)\n",
    "    layers.append(input_layer)\n",
    "    if batchnorm:\n",
    "        x = BatchNorm1d(n_hidden_per_layer, device=device)\n",
    "        layers.append(x)\n",
    "    if tanh:\n",
    "        x = Tanh()\n",
    "        layers.append(x)\n",
    "\n",
    "    # setup hidden\n",
    "    for _ in range(num_hidden_layers):\n",
    "        x = Linear(n_hidden_per_layer, n_hidden_per_layer, bias=bias, device=device)\n",
    "        hidden_layers.append(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNorm1d(n_hidden_per_layer, device=device)\n",
    "            hidden_layers.append(x)\n",
    "        if tanh:\n",
    "            x = Tanh()\n",
    "            hidden_layers.append(x)\n",
    "\n",
    "    layers.extend(hidden_layers)\n",
    "\n",
    "    # setup output layer\n",
    "    output_layer = Linear(n_hidden_per_layer, vocab_size, bias=bias, device=device)\n",
    "    layers.append(output_layer)\n",
    "\n",
    "    # Now lets play with the gain a bit to see how it affects activations and gradients\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # make last layer less confident for starting with uniform distribution\n",
    "        output_layer.weights *= output_gain\n",
    "\n",
    "        if tanh:\n",
    "            for layer in hidden_layers: # if linear layers are followed by tanh, apply tanh gain\n",
    "                if isinstance(layer, Linear):\n",
    "                    layer.weights *= tanh_gain\n",
    "\n",
    "    parameters = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "    print(f\"Total number of parameters in model: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return layers, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16173917",
   "metadata": {},
   "source": [
    "Lets also port over our training model code with a few changes -\n",
    "\n",
    "1. We need to tell pytorch to retain gradients for the intermediate (non leaf tensors). This is because by default, only leaf tensors (tensors explicitly created by the user or not the result of an operation) that have requires_grad=True retain their gradients. We want to access these grandients layer for diagnostics.\n",
    "\n",
    "2. The forward pass looks much simpler now, we simple go through each layer sequentially\n",
    "\n",
    "3. One important diagnostic metric that we want to track is how big is the update to a parameter. We know that in each backward step, we reduce the data of a parameter by the gradient scaled by a learning rate. The ratio of this update to the data will tell us how big or small the update is w.r.t to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d507e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(C, layers, parameters, n_steps = 200000, batch_size = 32, lr = 0.1, decayed_lr = 0.01):\n",
    "\n",
    "    stepi = []\n",
    "    lossi = []\n",
    "    loglossi = []\n",
    "    update_to_data_ratio = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for i in pbar:\n",
    "        # Mini-batch: randomly sample 32 examples\n",
    "        ixds = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n",
    "\n",
    "        # Forward pass\n",
    "        emb = C[mini_batch_inp]\n",
    "        x = emb.view(emb.shape[0], -1)\n",
    "\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # loss function\n",
    "        loss = F.cross_entropy(x, mini_batch_target)\n",
    "\n",
    "        # Backward pass\n",
    "\n",
    "        # Retain gradients for intermediate layers for diagnostics\n",
    "        for layer in layers:\n",
    "            layer.out.retain_grad()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Learning rate step decay: 0.1 for first 100K steps, then 0.01\n",
    "        lr = lr if i < 100000 else decayed_lr\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "        # Track loss\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.item())\n",
    "        loglossi.append(loss.log10().item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.data:.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in parameters:\n",
    "\n",
    "                update_std = lr * p.grad.std()\n",
    "                data_std = p.data.std().log10().item()\n",
    "                ratio = update_std / data_std\n",
    "                update_to_data_ratio.append(ratio)\n",
    "\n",
    "    return stepi, lossi, loglossi, update_to_data_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c81dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complete_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
