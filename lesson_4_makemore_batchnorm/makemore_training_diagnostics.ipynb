{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705158f5",
   "metadata": {},
   "source": [
    "In this notebook, we will be working through [Building makemore Part 2: Training Diagnostics](https://www.youtube.com/watch?v=P6sfmUTpUmc) by Andrej Karpathy. This is the fourth video in the \"Neural Networks: Zero to Hero\" series and covers part 2 of it.\n",
    "\n",
    "In this video we will look at diagnostic tools that we can use to see if our models are training optimally.\n",
    "\n",
    "**Why training diagnostics matter:** When a neural network underperforms, the loss number alone doesn't tell you _why_. Is the learning rate too high? Are gradients vanishing in deep layers? Are neurons saturating? Training diagnostics give us visibility into the internal dynamics of the network — activations, gradients, and parameter updates — so we can distinguish between a model that's learning healthily and one that's silently broken. This is especially important as networks get deeper, where problems like vanishing/exploding gradients can make training appear to work (the loss decreases) while the network is actually only learning in a subset of its layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e0a53",
   "metadata": {},
   "source": [
    "# Section 1: \"Pytorchify\" Our Model\n",
    "\n",
    "In the previous notebook, we had a very haphazard way of creating the model. Lets first package those into layers similar to how pytorch does it.\n",
    "\n",
    "The idea here is to move from \"raw tensor math\" to **modular, composable layers** — each layer is a callable object that stores its own parameters and output. This is exactly the design pattern PyTorch uses with `torch.nn.Module`: each layer knows how to do its forward pass, what parameters it owns, and (via autograd) how to compute gradients. By adopting this pattern now, we accomplish two things:\n",
    "\n",
    "1. **Cleaner code** — the forward pass becomes a simple loop through layers instead of a wall of matrix multiplications\n",
    "2. **Diagnostic access** — each layer stores `self.out`, so after training we can inspect activations and gradients at every point in the network\n",
    "\n",
    "Note that our custom classes below are simplified versions of PyTorch's `nn.Linear`, `nn.BatchNorm1d`, and `nn.Tanh`. They don't inherit from `nn.Module` (so no automatic parameter registration, `.to(device)`, `.eval()` mode toggling, etc.), but the core math is identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8253b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75d359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 names loaded\n",
      "Examples: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "Vocabulary size: 27\n",
      "Mappings: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# lets copy over some stuff from the previous notebook\n",
    "\n",
    "# utility to read dataset\n",
    "DATASET_PATH = '../names.txt'\n",
    "SPECIAL_TOKEN = \".\"\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_dataset():\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        rows = [row.strip() for row in f.readlines()]\n",
    "    return rows\n",
    "\n",
    "# Load dataset\n",
    "words = get_dataset()\n",
    "print(f\"{len(words)} names loaded\")\n",
    "print(f\"Examples: {words[:8]}\")\n",
    "\n",
    "# Build character mappings — identical to lesson 2\n",
    "# '.' is our special start/end token at index 0, then a=1, b=2, ..., z=26\n",
    "all_characters = [SPECIAL_TOKEN] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(all_characters)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Mappings: {itos}\")\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81272af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   182,625 examples from 25,626 names\n",
      "Validation:  22,655 examples from 3,203 names\n",
      "Test:        22,866 examples from 3,204 names\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words, block_size=3):\n",
    "    \"\"\"\n",
    "    Convert a list of words into (X, Y) tensors for training.\n",
    "\n",
    "    X shape: (N, block_size) — each row is a context window of character indices\n",
    "    Y shape: (N,) — each element is the target character index\n",
    "\n",
    "    This function will be called three times: once each for train, val, and test splits.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + SPECIAL_TOKEN:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 80/10/10 split at the word level\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size=3)     # training\n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size=3)  # validation (\"dev\" set)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size=3)      # test\n",
    "\n",
    "Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "Xdev, Ydev = Xdev.to(device), Ydev.to(device)\n",
    "Xte, Yte = Xte.to(device), Yte.to(device)\n",
    "\n",
    "print(f\"Training:   {Xtr.shape[0]:>7,} examples from {n1:,} names\")\n",
    "print(f\"Validation: {Xdev.shape[0]:>7,} examples from {n2-n1:,} names\")\n",
    "print(f\"Test:       {Xte.shape[0]:>7,} examples from {len(words)-n2:,} names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a121802",
   "metadata": {},
   "source": [
    "### Reference: old-style model from the previous notebook\n",
    "\n",
    "The cell below contains the raw-tensor implementation from the previous notebook (batch norm lesson). It's kept here commented-out as a reference point so we can compare the \"before\" and \"after\" as we refactor into modular layers. Notice how the forward pass is a single monolithic block of operations — by the end of this section, we'll replace that with a clean `for layer in layers` loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbd09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is the code we had at the end of the previous notebook\n",
    "\n",
    "def get_params(embed_dim, block_size, n_hidden):\n",
    "\n",
    "    tanh_gain = 5./3  # correction factor for tanh squashing variance\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C  = torch.randn((vocab_size, embed_dim),            generator=g)\n",
    "    W1 = torch.randn((embed_dim * block_size, n_hidden), generator=g) * (tanh_gain / (embed_dim * block_size) ** 0.5)  # Kaiming with tanh gain\n",
    "    b1 = torch.randn(n_hidden,                        generator=g) * 0.01  # small, near zero\n",
    "    W2 = torch.randn((n_hidden, vocab_size),           generator=g) * (1 / (n_hidden) ** 0.5)  # Xavier (gain=1, no activation)\n",
    "    b2 = torch.randn(vocab_size,                       generator=g) * 0.01  # small, near zero\n",
    "\n",
    "    bngain = torch.ones((1, n_hidden))\n",
    "    bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "    # add tracking variables which are NOT part of the training and thus dont receive gradients\n",
    "    bnstd_running = torch.ones((1, n_hidden))\n",
    "    bnmean_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "    parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return bnstd_running, bnmean_running, parameters\n",
    "\n",
    "def train_model(params, n_steps=200000, batch_size=32):\n",
    "\n",
    "    bnstd_running, bnmean_running, parameters = params\n",
    "    C, W1, b1, W2, b2, bngain, bnbias = parameters\n",
    "\n",
    "    stepi = []\n",
    "    lossi = []\n",
    "    loglossi = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for i in pbar:\n",
    "        # Mini-batch: randomly sample 32 examples\n",
    "        ixds = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n",
    "\n",
    "        # Forward pass\n",
    "        emb = C[mini_batch_inp]                                        # (32, 3, 10)\n",
    "        emb_cat = emb.view(emb.shape[0], -1)                            # (32, 30)\n",
    "        hidden_layer_preactivation = emb_cat @ W1 + b1                  # (32, 200)\n",
    "\n",
    "        # CONVERT hidden_layer_preactivation to unit gaussian\n",
    "        _mean = hidden_layer_preactivation.mean(axis=0, keepdim=True) # (1, 200) take mean across the samples in the mini batch\n",
    "        _std = hidden_layer_preactivation.std(axis=0, keepdim=True) # (1, 200) take std across the samples in the mini batch\n",
    "\n",
    "        hidden_layer_preactivation = (hidden_layer_preactivation - _mean) / _std # convert to unit gaussian\n",
    "\n",
    "        # move the running average slightly based on the currnet mean and std directions\n",
    "        with torch.no_grad():\n",
    "            bnmean_running = 0.999 * bnmean_running + 0.001 * _mean\n",
    "            bnstd_running = 0.999 * bnstd_running + 0.001 * _std\n",
    "\n",
    "        # scale and shift\n",
    "        hidden_layer_preactivation = hidden_layer_preactivation*bngain + bnbias\n",
    "\n",
    "        h = torch.tanh(hidden_layer_preactivation)  # (32, 200)\n",
    "        logits = h @ W2 + b2                                           # (32, 27)\n",
    "        loss = F.cross_entropy(logits, mini_batch_target)\n",
    "\n",
    "        # Backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Learning rate step decay: 0.1 for first 100K steps, then 0.01\n",
    "        lr = 0.1 if i < 100000 else 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "        # Track loss\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.item())\n",
    "        loglossi.append(loss.log10().item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.data:.4f}\")\n",
    "\n",
    "    return stepi, lossi, loglossi, hidden_layer_preactivation, h, logits, bnmean_running, bnstd_running\n",
    "\n",
    "# now we can use these running mean and std during eval\n",
    "@torch.no_grad()\n",
    "def eval_loss(X, Y, params, embed_dim, block_size):\n",
    "\n",
    "    bnstd_running, bnmean_running, parameters = params\n",
    "    C, W1, b1, W2, b2, bngain, bnbias = parameters\n",
    "\n",
    "    emb = C[X]\n",
    "    emb_cat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    hidden_layer_preactivation = emb_cat @ W1 + b1\n",
    "    hidden_layer_preactivation = (hidden_layer_preactivation - bnmean_running) / bnstd_running\n",
    "    hidden_layer_preactivation = hidden_layer_preactivation*bngain + bnbias\n",
    "\n",
    "    h = torch.tanh(hidden_layer_preactivation)  # (32, 200)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()\n",
    "\n",
    "# bnstd_running, bnmean_running, parameters = get_params(embed_dim, block_size, n_hidden)\n",
    "\n",
    "# stepi, lossi, loglossi, hidden_layer_preactivation, h, logits, bnmean_running, bnstd_running = \\\n",
    "#     train_model([bnstd_running, bnmean_running, parameters])\n",
    "\n",
    "# print(f\"Final Training loss: {eval_loss(Xtr, Ytr,[bnstd_running, bnmean_running, parameters], embed_dim, block_size)}, \\\n",
    "#     Validation Loss: {eval_loss(Xdev, Ydev, [bnstd_running, bnmean_running, parameters], embed_dim, block_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zqo7k8u2n6",
   "metadata": {},
   "source": [
    "### `Linear` — our version of `torch.nn.Linear`\n",
    "\n",
    "This layer computes `y = x @ W + b`. The key design choices:\n",
    "\n",
    "- **Kaiming initialization** — we divide the random weights by `√fan_in` (the number of input features). This keeps the variance of the output roughly equal to the variance of the input, preventing activations from exploding or vanishing as data flows through the network. This is the same initialization PyTorch uses by default for `nn.Linear`.\n",
    "- **`self.out`** — we store the output on the layer object so we can inspect it later for diagnostics. This is not something PyTorch does by default (you'd use forward hooks for that), but it's very convenient for learning.\n",
    "- **No bias by default in some configurations** — when BatchNorm follows a Linear layer, the bias is redundant because BatchNorm subtracts the mean anyway (and then adds its own learnable bias `β`). That's why `setup_model` can pass `bias=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8385e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Layer\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, num_input_features, num_output_features, bias=True, device='cpu'):\n",
    "\n",
    "        initialization_factor = num_input_features ** 0.5 # kaiming he initialization\n",
    "\n",
    "        self.weights = \\\n",
    "            torch.randn((num_input_features, num_output_features), generator=g, device=device) / initialization_factor\n",
    "        self.bias = torch.zeros(num_output_features, device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights # note that we keep the output under self so that we can access it later for diagnostics\n",
    "        if self.bias:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias] if self.bias else [self.weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pqm4vpv0vm",
   "metadata": {},
   "source": [
    "### `BatchNorm1d` — our version of `torch.nn.BatchNorm1d`\n",
    "\n",
    "Batch normalization, introduced in [Ioffe & Szegedy 2015](https://arxiv.org/abs/1502.03167), normalizes each feature across the mini-batch to have zero mean and unit variance, then applies a learnable scale (`γ`) and shift (`β`). Key things to note:\n",
    "\n",
    "- **Training vs. inference behavior** — during training, we compute mean/variance from the current mini-batch. During inference, we use the exponential moving averages (EMAs) accumulated during training. This dual behavior is why there's a `self.training` flag — and why in PyTorch you must call `model.eval()` before inference.\n",
    "- **Why EMA and not just a simple average?** — We could average all training-set statistics at the end, but EMA is computed on-the-fly and adapts as the model's parameters change during training. The EMA with momentum 0.1 means each update blends 10% of the current batch stats with 90% of the running estimate.\n",
    "- **`eps`** — a small constant (1e-5) added inside the square root to prevent division by zero when variance is near zero.\n",
    "- **Why BatchNorm helps** — it reduces \"internal covariate shift\" (the original motivation), but more practically it acts as a form of regularization (each sample's normalization depends on what other samples are in the batch — a source of noise) and allows higher learning rates by keeping activations in a well-behaved range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fd0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batchnorm layer\n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, num_input_features, momentum=0.1, eps=1e-5, device='cpu'):\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # learnable scale and shift parameters (gain and bias)\n",
    "        self.gamma = torch.ones(num_input_features, device=device)\n",
    "        self.beta = torch.zeros(num_input_features, device=device)\n",
    "\n",
    "        # Exponential moving average (EMA) tracking\n",
    "        self.running_mean = torch.zeros(num_input_features, device=device)\n",
    "        self.runnins_vars = torch.ones(num_input_features, device=device)\n",
    "\n",
    "        self.training = True # batchnorm behaves different during training vs during inference/eval\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "\n",
    "        if self.training:\n",
    "            xmean = x.mean(axis=0, keepdim=True) # mean across batch\n",
    "            xvars = x.var(axis=0, keepdim=True) # variance across batch\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvars = self.runnins_vars\n",
    "\n",
    "        # normalize\n",
    "        self.out = (x - xmean) / torch.sqrt(xvars + self.eps)\n",
    "\n",
    "        # scale and shift\n",
    "        self.out = self.gamma * self.out + self.beta\n",
    "\n",
    "        # EMA update\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad(): # these are not learnable params\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + xmean * self.momentum\n",
    "                self.running_vars = self.running_vars * (1 - self.momentum) + xvars * self.momentum\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jdw0phe74sl",
   "metadata": {},
   "source": [
    "### `Tanh` — activation function layer\n",
    "\n",
    "Wrapping `torch.tanh` in a class might seem like overkill, but it serves the same purpose as the other layers: it stores `self.out` so we can inspect post-activation values during diagnostics. Later we'll use this to check for **saturation** — if too many tanh outputs are near ±1, gradients in those regions are near zero, and learning stalls (the \"dead neuron\" problem for tanh).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75c26493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf504rha74d",
   "metadata": {},
   "source": [
    "### `setup_model` — composing layers into a network\n",
    "\n",
    "This function builds the full network as a flat list of layers: `[Linear → BatchNorm → Tanh] × N → Linear`. This is equivalent to `torch.nn.Sequential` in PyTorch. A few important details:\n",
    "\n",
    "- **`output_gain = 0.1`** — the output layer's weights are scaled down by 10×, so the initial logits are small and the softmax distribution starts nearly uniform. Without this, the model would start overconfident on random classes, producing a high initial loss and large gradients that can destabilize early training.\n",
    "- **`tanh_gain = 5/3 ≈ 1.667`** — this compensates for tanh squashing the variance of its output. If input to tanh is standard normal, the output variance is less than 1 (because tanh compresses values toward ±1). The gain factor `5/3` is the empirically-derived correction from the [Kaiming He paper](https://arxiv.org/abs/1502.01852) that keeps the variance roughly stable through tanh layers. Note: this gain is applied _on top of_ the Kaiming `1/√fan_in` already in the `Linear` class.\n",
    "- **Why separate `hidden_layers`?** — the gain adjustment only applies to Linear layers followed by tanh, _not_ to the output layer (which has no activation). Separating them lets us selectively apply the gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fn to create a stack of layers\n",
    "\n",
    "def setup_model(embed_dim,\n",
    "                block_size,\n",
    "                n_hidden_per_layer,\n",
    "                num_hidden_layers = 4,\n",
    "                bias = True,\n",
    "                tanh = True,\n",
    "                batchnorm = True,\n",
    "                output_gain = 0.1,\n",
    "                tanh_gain = 5./3,\n",
    "                device = 'cpu'\n",
    "                ):\n",
    "\n",
    "    # create embedding lookup\n",
    "\n",
    "    C = torch.randn((vocab_size, embed_dim), generator=g, device=device)\n",
    "\n",
    "    flattened_input_features = embed_dim * block_size\n",
    "\n",
    "    layers = []\n",
    "    hidden_layers = []\n",
    "    # setup input\n",
    "\n",
    "    input_layer = Linear(flattened_input_features, n_hidden_per_layer, bias=bias, device=device)\n",
    "    layers.append(input_layer)\n",
    "    if batchnorm:\n",
    "        x = BatchNorm1d(n_hidden_per_layer, device=device)\n",
    "        layers.append(x)\n",
    "    if tanh:\n",
    "        x = Tanh()\n",
    "        layers.append(x)\n",
    "\n",
    "    # setup hidden\n",
    "    for _ in range(num_hidden_layers):\n",
    "        x = Linear(n_hidden_per_layer, n_hidden_per_layer, bias=bias, device=device)\n",
    "        hidden_layers.append(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNorm1d(n_hidden_per_layer, device=device)\n",
    "            hidden_layers.append(x)\n",
    "        if tanh:\n",
    "            x = Tanh()\n",
    "            hidden_layers.append(x)\n",
    "\n",
    "    layers.extend(hidden_layers)\n",
    "\n",
    "    # setup output layer\n",
    "    output_layer = Linear(n_hidden_per_layer, vocab_size, bias=bias, device=device)\n",
    "    layers.append(output_layer)\n",
    "\n",
    "    # Now lets play with the gain a bit to see how it affects activations and gradients\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # make last layer less confident for starting with uniform distribution\n",
    "        output_layer.weights *= output_gain\n",
    "\n",
    "        if tanh:\n",
    "            for layer in hidden_layers: # if linear layers are followed by tanh, apply tanh gain\n",
    "                if isinstance(layer, Linear):\n",
    "                    layer.weights *= tanh_gain\n",
    "\n",
    "    parameters = [C] + [param for layer in layers for param in layer.parameters()]\n",
    "    print(f\"Total number of parameters in model: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return layers, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16173917",
   "metadata": {},
   "source": [
    "## Training loop with diagnostics\n",
    "\n",
    "Lets also port over our training model code with a few changes -\n",
    "\n",
    "1. **Retaining intermediate gradients** — By default, PyTorch only keeps gradients for \"leaf\" tensors (parameters you created directly). Intermediate results (like the output of a hidden layer) have their gradients discarded after `backward()` to save memory. Calling `retain_grad()` on `layer.out` tells PyTorch to keep those gradients so we can inspect them later. In production code, you'd use **forward/backward hooks** (`register_forward_hook`, `register_full_backward_hook`) instead, but `retain_grad()` is simpler for learning.\n",
    "\n",
    "2. **Cleaner forward pass** — instead of manually writing out each matrix multiplication, we loop through our layer objects: `for layer in layers: x = layer(x)`. This is exactly what `nn.Sequential.__call__` does internally.\n",
    "\n",
    "3. **Update-to-data ratio** — This is the key diagnostic Karpathy introduces. For each parameter tensor, we compute:\n",
    "\n",
    "$$\\text{ratio} = \\frac{\\text{std}(\\text{lr} \\times \\nabla p)}{\\text{std}(p)}$$\n",
    "\n",
    "This tells us how large each gradient update is _relative to the parameter values themselves_. Karpathy's rule of thumb: this ratio should be roughly **1e-3** (i.e., the update is about 1/1000th the size of the parameter). If it's much larger, learning is unstable (parameters are being yanked around too aggressively). If it's much smaller, the parameters are barely changing and learning is too slow. This is a much more informative signal than just watching the loss curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d507e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(C, layers, parameters, n_steps = 200000, batch_size = 32, lr = 0.1, decayed_lr = 0.01):\n",
    "\n",
    "    stepi = []\n",
    "    lossi = []\n",
    "    loglossi = []\n",
    "    update_to_data_ratio = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for i in pbar:\n",
    "        # Mini-batch: randomly sample 32 examples\n",
    "        ixds = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n",
    "\n",
    "        # Forward pass\n",
    "        emb = C[mini_batch_inp]\n",
    "        x = emb.view(emb.shape[0], -1)\n",
    "\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # loss function\n",
    "        loss = F.cross_entropy(x, mini_batch_target)\n",
    "\n",
    "        # Backward pass\n",
    "\n",
    "        # Retain gradients for intermediate layers for diagnostics\n",
    "        for layer in layers:\n",
    "            layer.out.retain_grad()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Learning rate step decay: 0.1 for first 100K steps, then 0.01\n",
    "        lr = lr if i < 100000 else decayed_lr\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "\n",
    "        # Track loss\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.item())\n",
    "        loglossi.append(loss.log10().item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix(loss=f\"{loss.data:.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in parameters:\n",
    "\n",
    "                update_std = lr * p.grad.std()\n",
    "                data_std = p.data.std().log10().item()\n",
    "                ratio = update_std / data_std\n",
    "                update_to_data_ratio.append(ratio)\n",
    "\n",
    "    return stepi, lossi, loglossi, update_to_data_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c81dc",
   "metadata": {},
   "source": [
    "# Section 2: Creating Diagnostic Plots\n",
    "\n",
    "Here we create a few diagnostic plots which will help with understanding how well a model is training.\n",
    "\n",
    "These are the four core diagnostics Karpathy walks through in the video. Together, they give a complete picture of the network's health:\n",
    "\n",
    "| Diagnostic                       | What it shows                                                | What to look for                                                                                             |\n",
    "| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **Activation distribution**      | Values flowing _forward_ through each layer                  | Should be roughly Gaussian, centered near 0. If values pile up at ±1, tanh is saturating.                    |\n",
    "| **Gradient distribution**        | Values flowing _backward_ through each layer                 | Should have similar scale across layers. If gradients shrink in earlier layers → vanishing gradient problem. |\n",
    "| **Weight gradient distribution** | How much each weight matrix wants to change                  | Should be roughly the same order of magnitude across layers.                                                 |\n",
    "| **Update-to-data ratio**         | Size of the actual parameter update vs. the parameter itself | Should hover around 1e-3. Much larger → unstable. Much smaller → learning too slowly.                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gi8ot1sp13c",
   "metadata": {},
   "source": [
    "### Activation distribution\n",
    "\n",
    "This plots a histogram of the output values at each Tanh layer. We're checking for **saturation**: when tanh outputs cluster near ±1 (above the 0.97 threshold), those neurons are in the flat region of tanh where the gradient is nearly zero. A saturated neuron can barely learn — it's effectively \"dead\" for that training step.\n",
    "\n",
    "**Healthy activations** look like a smooth bell curve between -1 and +1 with most values well inside the ±0.97 range. **Unhealthy activations** show a bimodal distribution with peaks at ±1 (the \"U-shape\"), meaning the pre-activation values are too large and tanh is squashing everything to the extremes. This is exactly the kind of problem that proper weight initialization (Kaiming) and BatchNorm are designed to prevent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_distribution(layers, layer_to_plot = Tanh):\n",
    "\n",
    "    # visualize histograms\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "        if isinstance(layer, layer_to_plot):\n",
    "            t = layer.out # this is the activation of the layer\n",
    "            layer_name = layer.__class__.__name__\n",
    "            saturation_count = (t.abs() > 0.97).float().mean()*100\n",
    "\n",
    "            print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer_name, t.mean(), t.std(), saturation_count))\n",
    "\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "    plt.legend(legends);\n",
    "    plt.title('activation distribution')\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791amxp1d9h",
   "metadata": {},
   "source": [
    "### Activation Gradient distribution\n",
    "\n",
    "This is the backward-pass counterpart to the activation plot above. Instead of looking at `layer.out`, we look at `layer.out.grad` — the gradient of the loss with respect to each layer's output.\n",
    "\n",
    "**What we're watching for:** the gradients should have roughly similar standard deviations across all layers. If the std shrinks dramatically in earlier layers (closer to the input), we have a **vanishing gradient problem** — those layers are receiving almost no learning signal. If it grows, we have **exploding gradients**.\n",
    "\n",
    "In a well-initialized network with BatchNorm, the gradient magnitudes should be relatively uniform across layers, because BatchNorm helps stabilize both the forward and backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21dad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_distribution(layers, layer_to_plot = Tanh):\n",
    "\n",
    "    # visualize histograms\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "        if isinstance(layer, layer_to_plot):\n",
    "            t = layer.out.grad # this is the gradient flowing through the layer\n",
    "            layer_name = layer.__class__.__name__\n",
    "\n",
    "            print('layer %d (%10s): mean %+.2f, std %.2f' % (i, layer_name, t.mean(), t.std()))\n",
    "\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "    plt.legend(legends);\n",
    "    plt.title('activation distribution')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tgqqhxnwm0r",
   "metadata": {},
   "source": [
    "### Weight gradient distribution\n",
    "\n",
    "While the previous plot showed gradients flowing through activations, this one looks at the gradients on the **weight matrices themselves** — i.e., `p.grad` for each 2D parameter tensor. This tells us how much each weight matrix \"wants\" to change.\n",
    "\n",
    "The key metric printed is the **grad:data ratio** (`grad.std() / param.std()`). If one layer's weights have gradients that are orders of magnitude smaller than other layers', that layer is learning much more slowly — a sign of poor initialization or vanishing gradients reaching that layer.\n",
    "\n",
    "Note: we only plot 2D parameters (weight matrices), skipping 1D parameters (biases, BatchNorm gamma/beta) since the weight matrices contain the bulk of the model's capacity and are more informative for diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_gradient_distribution(parameters):\n",
    "\n",
    "    plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "    legends = []\n",
    "    for i,p in enumerate(parameters):\n",
    "        t = p.grad\n",
    "        if p.ndim == 2:\n",
    "            print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "            hy, hx = torch.histogram(t, density=True)\n",
    "            plt.plot(hx[:-1].detach(), hy.detach())\n",
    "            legends.append(f'{i} {tuple(p.shape)}')\n",
    "    plt.legend(legends)\n",
    "    plt.title('weights gradient distribution');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jf3a3ze7488",
   "metadata": {},
   "source": [
    "### Update-to-data ratio over time\n",
    "\n",
    "This is arguably the most important diagnostic. It plots, for each weight matrix, the ratio of `(lr × grad.std()) / param.std()` across all training steps.\n",
    "\n",
    "The black horizontal line at -3 (on the log10 scale) marks the **1e-3 sweet spot**. Karpathy's heuristic:\n",
    "\n",
    "- **Ratio ≈ 1e-3** — healthy. Each update nudges the parameters by about 0.1% of their magnitude.\n",
    "- **Ratio >> 1e-3** (e.g., 1e-1) — the learning rate is too high for this layer, causing large, destabilizing jumps.\n",
    "- **Ratio << 1e-3** (e.g., 1e-5) — this layer is barely learning. Either the learning rate is too low or gradients are vanishing before they reach this layer.\n",
    "\n",
    "Unlike the loss curve, this diagnostic tells you _which layers_ are learning too fast or too slowly, letting you diagnose initialization and learning rate problems layer by layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b3738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_update_to_data_ratio_distribution(parameters, update_to_data_ratio):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    for i,p in enumerate(parameters):\n",
    "        if p.ndim == 2:\n",
    "            plt.plot([update_to_data_ratio[j][i] for j in range(len(update_to_data_ratio))])\n",
    "            legends.append('param %d' % i)\n",
    "    plt.plot([0, len(update_to_data_ratio)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "    plt.legend(legends);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7880b",
   "metadata": {},
   "source": [
    "### Aside: Why plots for both activation gradient and weight gradient?\n",
    "\n",
    "When watching the video, I was a little confused as to why we need plots for both. Here is the reason —\n",
    "\n",
    "They measure **different quantities** that can diverge from each other. To see why, consider what happens in a single Linear layer computing `h = x @ W`:\n",
    "\n",
    "- **`layer.out.grad`** = `∂L/∂h` — the gradient of the loss with respect to the layer's **output activation**. This is the signal flowing backward through the network.\n",
    "- **`W.grad`** = `∂L/∂W` — the gradient of the loss with respect to the **weight matrix**. This is what actually gets used in the SGD update step.\n",
    "\n",
    "The chain rule connects them:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = x^T \\cdot \\frac{\\partial L}{\\partial h}$$\n",
    "\n",
    "The weight gradient is the **product of two things**: the input `x` from the forward pass, and the activation gradient `∂L/∂h` from the backward pass. So even if the activation gradients are perfectly healthy, the weight gradients can be off if the forward-pass inputs have unusual scale.\n",
    "\n",
    "#### Concrete example with a 3-layer network\n",
    "\n",
    "Consider `h1 = x @ W1`, `h2 = h1 @ W2`, `h3 = h2 @ W3`.\n",
    "\n",
    "The **activation gradients** (plot 2) flow backward:\n",
    "\n",
    "```\n",
    "∂L/∂h3  →  ∂L/∂h2  →  ∂L/∂h1\n",
    "```\n",
    "\n",
    "This plot checks: _is the backward signal staying stable as it flows from the output back to the input?_\n",
    "\n",
    "The **weight gradients** (plot 3) are:\n",
    "\n",
    "```\n",
    "W1.grad = x^T   @ (∂L/∂h1)\n",
    "W2.grad = h1^T  @ (∂L/∂h2)\n",
    "W3.grad = h2^T  @ (∂L/∂h3)\n",
    "```\n",
    "\n",
    "Each weight gradient depends on **both** the backward gradient **and** the forward input to that layer.\n",
    "\n",
    "#### When they diverge\n",
    "\n",
    "Suppose the activation gradients all have similar standard deviations (plot 2 looks healthy). But `h1` has very small values — maybe tanh squashed the activations down. Then:\n",
    "\n",
    "- `W2.grad = h1^T @ (∂L/∂h2)` would be **tiny** — because `h1` is tiny, even though `∂L/∂h2` is fine\n",
    "- `W3.grad = h2^T @ (∂L/∂h3)` might be normal\n",
    "\n",
    "Plot 2 (activation gradients) wouldn't catch this — the backward signal is fine. But plot 3 (weight gradients) would immediately show that `W2` has much smaller gradients than `W3`, meaning that layer is learning much more slowly.\n",
    "\n",
    "**In short:** the activation gradient plot diagnoses the health of the **backward pass**. The weight gradient plot diagnoses the health of the **actual learning signal**, which depends on both the backward pass _and_ the forward pass together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fad1f5",
   "metadata": {},
   "source": [
    "Section 3:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complete_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
