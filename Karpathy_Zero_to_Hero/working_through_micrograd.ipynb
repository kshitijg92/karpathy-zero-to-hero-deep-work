{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925bb3ad",
   "metadata": {},
   "source": [
    "In this notebook, we will be working through the micrograd [video](https://www.youtube.com/watch?v=VMj-3S1tku0) by the GOAT Andrej Karpathy. Goal is to not simply copy paste code, but to type each character by hand and wrangle everything to build an intuitive understanding.\n",
    "\n",
    "Derivatives are the mathematical foundation of how neural networks learn. When we train a neural network, we're trying to find the set of weights that minimise some loss function \u2014 a single number that measures how wrong our model is. The way we reduce that loss is by computing the derivative of the loss with respect to every weight in the network. That derivative tells us which direction to nudge each weight to make the loss go down. Do that repeatedly and the network learns. The algorithm that computes all those derivatives efficiently is called **backpropagation**, and micrograd will build it from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bea79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b4d7b",
   "metadata": {},
   "source": [
    "# Section 1: What is a derivate? Building intuition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075e7bd",
   "metadata": {},
   "source": [
    "## Simple function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73e1330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a simple sample function\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5\n",
    "\n",
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc5aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs: [-5.   -4.75 -4.5  -4.25 -4.   -3.75 -3.5  -3.25 -3.   -2.75 -2.5  -2.25\n",
      " -2.   -1.75 -1.5  -1.25 -1.   -0.75 -0.5  -0.25  0.    0.25  0.5   0.75\n",
      "  1.    1.25  1.5   1.75  2.    2.25  2.5   2.75  3.    3.25  3.5   3.75\n",
      "  4.    4.25  4.5   4.75]\n",
      "ys: [100.      91.6875  83.75    76.1875  69.      62.1875  55.75    49.6875\n",
      "  44.      38.6875  33.75    29.1875  25.      21.1875  17.75    14.6875\n",
      "  12.       9.6875   7.75     6.1875   5.       4.1875   3.75     3.6875\n",
      "   4.       4.6875   5.75     7.1875   9.      11.1875  13.75    16.6875\n",
      "  20.      23.6875  27.75    32.1875  37.      42.1875  47.75    53.6875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x78b07f394550>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let us draw the function\n",
    "xs = np.arange(-5, 5, 0.25) # let a bunch of points from -5 (inc) to 5 (not inc) at a step size of 0.25\n",
    "print(f\"xs: {xs}\")\n",
    "\n",
    "ys = f(xs)\n",
    "print(f\"ys: {ys}\")\n",
    "\n",
    "# plot it\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f50bcc",
   "metadata": {},
   "source": [
    "The function we defined above was a parabola. Now lets think about what is a derivative? A derivative represents the instantaneous rate of change of a function with respect to its variable. In layman terms it tells us how does the output (y in our case) vary when we make a very small (infinitesimally) change (h) in our input (x).  \n",
    "This is how its defined mathematically -\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "In practice we approximate this by choosing a very small but finite h. The choice of h is a balancing act: too large and the approximation is inaccurate (the definition assumes h\u21920), too small and floating point precision breaks down (computers can't represent arbitrarily small differences exactly, so the numerator becomes 0). A value around 1e-5 is typically a reliable sweet spot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55984fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets understand this with an example\n",
    "h = 0.000001\n",
    "x = 3.0 # when our input is at 3.0 and we slightly nudge it higher, by what\n",
    "# sensitity does the output y change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4c5fc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.000014000003002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x+h) # looking at our parabola this should be higher than f(3) which was 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2df670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4000003002223593e-05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x+h) - f(x) # change in y when we nudge x by h at x=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3446c525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.000003002223593"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(x+h) - f(x))/h # at what rate does y change (or how sensitive is it) when we nudge h slightly higher at x=3. this is the derivate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c97671",
   "metadata": {},
   "source": [
    "This derivate turns out to be 14, which matches the derivate of f(x). From calculas we know derivate of function\n",
    "\n",
    "$$\n",
    "f = 3x^2 - 4x + 5\n",
    "$$\n",
    "\n",
    "is\n",
    "\n",
    "$$\n",
    "f' = 6x - 4\n",
    "$$\n",
    "\n",
    "and f'(3) = 6\\*3 - 4 = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33635fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.00003000000538"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create a simple helper to calculate derivate at a point using the limit formula\n",
    "\n",
    "def derivate(func, x):\n",
    "    h = 0.00001\n",
    "    return (func(x+h) - func(x)) / h\n",
    "\n",
    "derivate(f, 3) # note that in python we can pass functions as arguments like so"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf5be7",
   "metadata": {},
   "source": [
    "Just to bring home the point, even visually looking at the parabola, we can see that when x=-3, nudging x higher by a small amount h will decrease y, thus we expect the derivate at -3 to negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5d428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-21.999970000052823"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivate(f, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hv5upx9tio",
   "metadata": {},
   "source": [
    "## Visualising the derivative as a tangent line\n",
    "\n",
    "Geometrically, the derivative at a point is the slope of the line that just touches the curve there \u2014 the **tangent line**. This is the clearest way to see what the derivative actually means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2zysobcyv2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "\n",
    "# note: we compute slope inline here because the variable name 'derivate' was\n",
    "# reused as a float in the partial derivatives cells above, shadowing the function\n",
    "def slope_at(func, x, h=1e-5):\n",
    "    return (func(x + h) - func(x)) / h\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, x0, color in zip(axes, [3.0, -3.0], ['tab:orange', 'tab:green']):\n",
    "    slope = slope_at(f, x0)\n",
    "    tangent = lambda x, x0=x0, slope=slope: f(x0) + slope * (x - x0)\n",
    "\n",
    "    ax.plot(xs, f(xs), label='f(x)', color='tab:blue')\n",
    "    ax.plot(xs, tangent(xs), '--', label=f'tangent at x={x0} (slope={slope:.1f})', color=color)\n",
    "    ax.scatter([x0], [f(x0)], color='red', zorder=5)\n",
    "    ax.set_ylim(-20, 100)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Derivative at x={x0}')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# At x=3: positive slope (14) \u2014 moving right increases f(x)\n",
    "# At x=-3: negative slope (-22) \u2014 moving right decreases f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1656cfe",
   "metadata": {},
   "source": [
    "## Slightly more complex function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2e0e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = a*b + c\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b715787",
   "metadata": {},
   "source": [
    "Now lets think through the derivates of d wrt the inputs a, b, and c.\n",
    "\n",
    "lets start with 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c116c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original d: 4.0\n",
      "New d on nudging a: 3.9999699999999994\n",
      "Derivate: -3.000000000064062\n"
     ]
    }
   ],
   "source": [
    "h = 0.00001\n",
    "\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = a*b + c\n",
    "d_change = (a+h)*b + c\n",
    "derivate = (d_change - d) / h\n",
    "\n",
    "print(f\"original d: {d}\")\n",
    "print(f\"New d on nudging a: {d_change}\") # this will be lower than d since we are increasing a while b is negative, this pushing d down\n",
    "print(f\"Derivate: {derivate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d742e71",
   "metadata": {},
   "source": [
    "This tells us that at this point in the function, when we nudge a, the value of d goes down and the slope/rate of that is -3. This matches what calculas tells us.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial a} = b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial b} = a\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial c} = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original d: 4.0\n",
      "New d on nudging b: 4.00002\n",
      "Derivate: 2.0000000000131024\n"
     ]
    }
   ],
   "source": [
    "# how does 'b' influence the output 'd'\n",
    "\n",
    "h = 0.00001\n",
    "\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = a*b + c\n",
    "d_change = a*(b+h) + c\n",
    "derivate = (d_change - d) / h\n",
    "\n",
    "print(f\"original d: {d}\")\n",
    "print(f\"New d on nudging b: {d_change}\")\n",
    "print(f\"Derivate: {derivate}\")  # should be equal to a from the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963ddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original d: 4.0\n",
      "New d on nudging c: 4.000000099999999\n",
      "Derivate: 0.999999993922529\n"
     ]
    }
   ],
   "source": [
    "# how does 'c' influence the output 'd'\n",
    "\n",
    "h = 0.0000001\n",
    "\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = a*b + c\n",
    "d_change = a*b + (c + h)\n",
    "derivate = (d_change - d) / h\n",
    "\n",
    "print(f\"original d: {d}\")\n",
    "print(f\"New d on nudging c: {d_change}\")\n",
    "print(f\"Derivate: {derivate}\") # should be equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bnsmfq2lcv8",
   "metadata": {},
   "source": [
    "## The Gradient\n",
    "\n",
    "When a function has multiple inputs, its derivative with respect to each input is called a **partial derivative** (written \u2202 instead of d, to signal \"this is one of several\"). We just computed three of them: \u2202d/\u2202a, \u2202d/\u2202b, \u2202d/\u2202c.\n",
    "\n",
    "Collect all the partial derivatives into a single vector and you have the **gradient**:\n",
    "\n",
    "$$\n",
    "\\nabla d = \\left[\\frac{\\partial d}{\\partial a},\\ \\frac{\\partial d}{\\partial b},\\ \\frac{\\partial d}{\\partial c}\\right] = [b,\\ a,\\ 1] = [-3,\\ 2,\\ 1]\n",
    "$$\n",
    "\n",
    "### Why does the gradient point towards steepest increase?\n",
    "\n",
    "Each partial derivative tells us: \"if I nudge this one input up by a tiny amount, how much does the output change?\" A large positive value means that input has a big upward pull on the output. A large negative value means it pulls the output down.\n",
    "\n",
    "The gradient vector packages all of those individual sensitivities together. When you move in the direction the gradient points \u2014 increasing each input proportionally to its partial derivative \u2014 every input is working in concert to push the output up as hard as possible. Any other direction would have at least one input \"fighting\" the others, producing less total increase.\n",
    "\n",
    "A concrete way to see this: imagine standing on a hilly landscape and taking one step of fixed length in any direction. The direction that gains you the most height is exactly where the ground is rising fastest under your feet \u2014 the gradient direction. Step perpendicular to the gradient and you stay at the same height (that is a **level curve**). Step against the gradient and you descend.\n",
    "\n",
    "For our example, the gradient is [-3, 2, 1]. This tells us:\n",
    "\n",
    "- Increasing `a` _decreases_ d (slope = -3, the steepest puller)\n",
    "- Increasing `b` _increases_ d (slope = 2)\n",
    "- Increasing `c` _increases_ d (slope = 1, weakest effect)\n",
    "\n",
    "So to increase d as fast as possible, decrease `a` and increase `b` and `c` \u2014 proportional to [-3, 2, 1].\n",
    "\n",
    "To minimise the function (i.e. reduce the loss in a neural network) we move in the **opposite** direction \u2014 this is exactly what gradient descent does:\n",
    "\n",
    "$$\n",
    "\\text{weight} \\leftarrow \\text{weight} - \\text{learning rate} \\times \\frac{\\partial \\text{loss}}{\\partial \\text{weight}}\n",
    "$$\n",
    "\n",
    "Every weight in a neural network is like `a`, `b`, or `c` here. Backpropagation is the algorithm that efficiently computes all those partial derivatives at once, by working backwards through the computation graph. That is what micrograd will implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210d036",
   "metadata": {},
   "source": [
    "# Section 2: Building Micrograd\n",
    "\n",
    "This section will cover building the micrograd engine for automatic differentiation \u2014 i.e. automatically calculating the derivative of a function w.r.t ALL of its parameters.\n",
    "\n",
    "In Section 1 we computed derivatives by hand using the limit formula, nudging each input by a small h and observing the change. That works fine for simple expressions, but a real neural network has millions of parameters. Computing derivatives one at a time, manually, is completely infeasible.\n",
    "\n",
    "**Automatic differentiation (autograd)** solves this. Instead of computing derivatives analytically or numerically, we build a system that:\n",
    "\n",
    "1. Records every mathematical operation as it happens (building a computation graph)\n",
    "2. Walks backwards through that graph, applying the chain rule at each step to efficiently compute the derivative of the output with respect to every single input \u2014 all at once\n",
    "\n",
    "PyTorch's `torch.Tensor` does exactly this under the hood. Micrograd is a from-scratch, minimal version of the same idea \u2014 small enough to understand completely, but identical in spirit to what powers modern deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac36f2",
   "metadata": {},
   "source": [
    "## Section 2.1 - The Value Class\n",
    "\n",
    "The key challenge with automatic differentiation is that Python's built-in numeric types (floats, ints) just compute values \u2014 they have no memory of how they got there. If you write `d = a * b + c`, Python evaluates that to a single float and throws away all information about `a`, `b`, and `c`.\n",
    "\n",
    "To backpropagate, we need to remember the full history of the computation: what operations were performed, and what inputs were involved at each step. That history forms a **Directed Acyclic Graph (DAG)** \u2014 a graph where each node is a value, and each edge points from an input to the output it helped create. \"Acyclic\" means no cycles \u2014 an output can't also be its own input, which would make the computation undefined.\n",
    "\n",
    "We build a custom `Value` class to wrap each number so we can attach this extra bookkeeping to it. Every time two `Value` objects interact (via addition, multiplication, etc.), the result is a new `Value` that remembers its parents and the operation that created it. Once we have the full graph, we can traverse it backwards from the output to compute gradients everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0518f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Value at 0x78b07f59af50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets start by defining a very simple value class\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "a = Value(2.0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bbe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets add a fn to pretty print the value class as opposed to the gibberish above\n",
    "# python allows overriding the __repr__ fn for this\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "a = Value(2.0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ee32aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Value' and 'Value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m a = Value(\u001b[32m2.0\u001b[39m)\n\u001b[32m      3\u001b[39m b = Value(-\u001b[32m3.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'Value' and 'Value'"
     ]
    }
   ],
   "source": [
    "# now we can see that at least we get a nice print output. Lets try adding two values\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9fb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-1.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this does not work because we havent told python how to add two 'Value' objects.\n",
    "# we can define that by overloading the __add__ fn\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95f5b0ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# what if we want to add integer?\u001b[39;00m\n\u001b[32m      2\u001b[39m a = Value(\u001b[32m2.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mValue.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     _sum = \u001b[38;5;28mself\u001b[39m.data + \u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Value(_sum)\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# what if we want to add integer?\n",
    "a = Value(2.0)\n",
    "a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e29d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that does not work because so far we were expecting addition to occur on two Value objects\n",
    "# let us fix that\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum)\n",
    "\n",
    "a = Value(2.0)\n",
    "a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9673fc86",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'Value'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# while a + 2 works, 2 + a doesnt because for a + 2 python will internally call a.__add__(2) which we just supported\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# however, when we do 2 + a, python will call 2.__add__(a), which is callig the add from the int object which does now know to add Value object\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# in python, the way to fix that is by overloading the __radd__ fn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'int' and 'Value'"
     ]
    }
   ],
   "source": [
    "# while a + 2 works, 2 + a doesnt because for a + 2 python will internally call a.__add__(2) which we just supported\n",
    "# however, when we do 2 + a, python will call 2.__add__(a), which is callig the add from the int object which does now know to add Value object\n",
    "# in python, the way to fix that is by overloading the __radd__ fn\n",
    "2 + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=4.0)\n",
      "Value(data=4.0)\n"
     ]
    }
   ],
   "source": [
    "# lets overlaod __radd__\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # we simply switch the other around since __add__ already handles adding int to Value\n",
    "        return self + other # equivalent to self.__add__(other)\n",
    "\n",
    "a = Value(2.0)\n",
    "print(a + 2)\n",
    "print(2 + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ae74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=4.0)\n",
      "Value(data=4.0)\n",
      "Value(data=6.0)\n",
      "Value(data=6.0)\n"
     ]
    }
   ],
   "source": [
    "# lets add similar fn for multiply now so we can recreate the d = a*b + c expression from section 1\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # we simply switch the other around since __add__ already handles adding int to Value\n",
    "        return self + other # equivalent to self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _product = self.data * other.data\n",
    "        return Value(_product)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # we simply switch the other around since __mul__ already handles adding int to Value\n",
    "        return self * other # equivalent to self.__mul__(other)\n",
    "\n",
    "a = Value(2.0)\n",
    "print(a + 2)\n",
    "print(2 + a)\n",
    "print(a*3)\n",
    "print(3*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d281b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets recreate our example now using Value object\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "d = a*b + c\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf325e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=4.0), {Value(data=-6.0), Value(data=10.0)})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, when we ask for derivates or gradients, we ask how does changing a change d.\n",
    "# we can imagine an expression or a fn as a DAG, in our case the node 'd' will have\n",
    "# pointers coming to it from 'a', 'b' and 'c'.\n",
    "\n",
    "# so we need to know what values depend on other values. We can keep track of this using\n",
    "# a children tuple\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data\n",
    "        self._prev = set(_children) # keep track of all objects which created this one\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum, _children = (self, other)) # add the children, here _sum was created by adding self and other\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # we simply switch the other around since __add__ already handles adding int to Value\n",
    "        return self + other # equivalent to self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _product = self.data * other.data\n",
    "        return Value(_product, (self, other))  # add the children, here _product was created by adding self and other\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # we simply switch the other around since __mul__ already handles adding int to Value\n",
    "        return self * other # equivalent to self.__mul__(other)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "d = a*b + c\n",
    "d, d._prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dag_explain_01",
   "metadata": {},
   "source": [
    "By tracking `_prev` (the children that created each node), we've built a **computation graph**. For the expression `d = a*b + c`, it looks like this:\n",
    "\n",
    "```\n",
    "a \u2500\u2500\u2510\n",
    "    \u251c\u2500\u2500[*]\u2500\u2500 (a*b) \u2500\u2500\u2510\n",
    "b \u2500\u2500\u2518                \u251c\u2500\u2500[+]\u2500\u2500 d\n",
    "c \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "Each node knows which operation created it and which nodes fed into it. This is exactly the structure we need for backpropagation: to compute how `d` changes with respect to `a`, we just follow the edges backwards from `d` through `+`, then through `*`, applying the chain rule at each step.\n",
    "\n",
    "This is why PyTorch builds a computation graph every time you do forward pass operations on tensors \u2014 it's recording the history it needs for `.backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=4.0), {Value(data=-6.0), Value(data=10.0)}, '+')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we know what children created this value, but we also need to keep track of what operation created this value\n",
    "# we need to keep track of the operation because gradients/derivates flowing through different operations are different\n",
    "# for .e.g da/dd turned out to be 'b' while da/dc turned out to be '1'.\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children) # keep track of all objects which created this one\n",
    "        self._op = _op\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _sum = self.data + other.data\n",
    "        return Value(_sum, (self, other), \"+\") # added the op\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # we simply switch the other around since __add__ already handles adding int to Value\n",
    "        return self + other # equivalent to self.__add__(other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n",
    "        _product = self.data * other.data\n",
    "        return Value(_product, (self, other), '*')  # added the op\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # we simply switch the other around since __mul__ already handles adding int to Value\n",
    "        return self * other # equivalent to self.__mul__(other)\n",
    "\n",
    "a = Value(2.0)\n",
    "b = Value(-3.0)\n",
    "c = Value(10.0)\n",
    "\n",
    "d = a*b + c\n",
    "d, d._prev, d._op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "op_explain_01",
   "metadata": {},
   "source": [
    "Knowing the operation (`_op`) that created each node is essential because **different operations have different gradient rules**.\n",
    "\n",
    "Recall from Section 1:\n",
    "\n",
    "- For `d = a*b + c`, the partial derivative `\u2202d/\u2202a = b` (came from the multiply)\n",
    "- The partial derivative `\u2202d/\u2202c = 1` (came from the add)\n",
    "\n",
    "When we walk backwards through the graph, at each node we need to know: \"what operation created me, so I can apply the right gradient formula?\" The chain rule says the gradient at each input equals the gradient flowing in from the output, multiplied by the local gradient of this operation. That local gradient formula is different for `*`, `+`, `tanh`, `exp`, etc.\n",
    "\n",
    "So `_op` tells the backward pass _which formula to use_ at each node. Without it, we'd have the graph structure but no way to know how to propagate gradients through it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182fa504",
   "metadata": {},
   "outputs": [],
   "source": "# We add a label field so each node can carry a human-readable name (e.g. 'a', 'b', 'L').\n# It defaults to '' so existing code that doesn't pass a label still works fine.\n# The label is purely for visualisation \u2014 it has no effect on the math.\n\nclass Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self._prev = set(_children) # keep track of all objects which created this one\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n        _sum = self.data + other.data\n        return Value(_sum, (self, other), \"+\") # added the op\n\n    def __radd__(self, other):\n        # we simply switch the other around since __add__ already handles adding int to Value\n        return self + other # equivalent to self.__add__(other)\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other) # create a Value object from other if it isnt\n        _product = self.data * other.data\n        return Value(_product, (self, other), '*')  # added the op\n\n    def __rmul__(self, other):\n        # we simply switch the other around since __mul__ already handles adding int to Value\n        return self * other # equivalent to self.__mul__(other)"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6d4457d",
   "metadata": {},
   "outputs": [],
   "source": "from graphviz import Digraph\n\ndef trace(root):\n    # DFS (depth-first search) to collect every node and edge in the graph.\n    # Starting from the root (output node), we recursively visit each node's\n    # _prev children, building up the full set of nodes and directed edges.\n    nodes, edges = set(), set()\n    def build(v):\n        if v not in nodes:\n            nodes.add(v)\n            for child in v._prev:\n                edges.add((child, v))  # edge goes from child -> parent (input -> output)\n                build(child)\n    build(root)\n    return nodes, edges\n\ndef draw_dot(root, format='svg', rankdir='LR'):\n    \"\"\"\n    format: png | svg | ...\n    rankdir: TB (top to bottom graph) | LR (left to right)\n    \"\"\"\n    assert rankdir in ['LR', 'TB']\n    nodes, edges = trace(root)\n    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n\n    for n in nodes:\n        # Each Value gets a rectangular 'record' node showing its label and data value.\n        dot.node(name=str(id(n)), label = \"{%s | data %.4f}\" % (n.label, n.data), shape='record')\n        if n._op:\n            # If this node was produced by an operation, we create a small extra node\n            # just for the op symbol (e.g. '+', '*'). This makes the graph easier to read\n            # by showing the operation explicitly, rather than cramming it inside the value node.\n            dot.node(name=str(id(n)) + n._op, label=n._op)\n            dot.edge(str(id(n)) + n._op, str(id(n)))  # op node -> value node\n\n    for n1, n2 in edges:\n        # Connect input nodes to the op node of their output (not directly to the output value node),\n        # so the graph reads: input -> op -> output\n        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n\n    return dot"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e98d7e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"916pt\" height=\"154pt\"\n",
       " viewBox=\"0.00 0.00 916.00 154.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 150)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-150 912,-150 912,4 -4,4\"/>\n",
       "<!-- 132699402023504 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>132699402023504</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"520,-109.5 520,-145.5 649,-145.5 649,-109.5 520,-109.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"531\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">f</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"542,-109.5 542,-145.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"595.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;2.0000</text>\n",
       "</g>\n",
       "<!-- 132699402010768* -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>132699402010768*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"712\" cy=\"-99.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"712\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 132699402023504&#45;&gt;132699402010768* -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>132699402023504&#45;&gt;132699402010768*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M649.35,-113.25C658.65,-111.18 667.92,-109.11 676.33,-107.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"677.13,-110.64 686.13,-105.05 675.6,-103.81 677.13,-110.64\"/>\n",
       "</g>\n",
       "<!-- 132699402010768 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>132699402010768</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"775,-81.5 775,-117.5 908,-117.5 908,-81.5 775,-81.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"788\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">L</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"801,-81.5 801,-117.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"854.5\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;8.0000</text>\n",
       "</g>\n",
       "<!-- 132699402010768*&#45;&gt;132699402010768 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>132699402010768*&#45;&gt;132699402010768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M739.11,-99.5C746.81,-99.5 755.66,-99.5 764.85,-99.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"764.92,-103 774.92,-99.5 764.92,-96 764.92,-103\"/>\n",
       "</g>\n",
       "<!-- 132699402024656 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>132699402024656</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-55.5 0,-91.5 133,-91.5 133,-55.5 0,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"13\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"26,-55.5 26,-91.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"79.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n",
       "</g>\n",
       "<!-- 132699402011536* -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>132699402011536*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"196\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 132699402024656&#45;&gt;132699402011536* -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>132699402024656&#45;&gt;132699402011536*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.12,-59.09C142.49,-57.03 151.8,-54.99 160.25,-53.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161.06,-56.54 170.08,-50.97 159.56,-49.7 161.06,-56.54\"/>\n",
       "</g>\n",
       "<!-- 132699402016976 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>132699402016976</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"520.5,-54.5 520.5,-90.5 648.5,-90.5 648.5,-54.5 520.5,-54.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"533.5\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"546.5,-54.5 546.5,-90.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"597.5\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 4.0000</text>\n",
       "</g>\n",
       "<!-- 132699402016976&#45;&gt;132699402010768* -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>132699402016976&#45;&gt;132699402010768*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M648.62,-86.08C658.08,-88.11 667.53,-90.15 676.1,-91.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"675.56,-95.46 686.08,-94.14 677.04,-88.61 675.56,-95.46\"/>\n",
       "</g>\n",
       "<!-- 132699402016976+ -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>132699402016976+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"457\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"457\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 132699402016976+&#45;&gt;132699402016976 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>132699402016976+&#45;&gt;132699402016976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M484,-72.5C491.81,-72.5 500.78,-72.5 510.08,-72.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"510.25,-76 520.25,-72.5 510.25,-69 510.25,-76\"/>\n",
       "</g>\n",
       "<!-- 132699402022096 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>132699402022096</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"259,-82.5 259,-118.5 394,-118.5 394,-82.5 259,-82.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"271\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"283,-82.5 283,-118.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"338.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 10.0000</text>\n",
       "</g>\n",
       "<!-- 132699402022096&#45;&gt;132699402016976+ -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>132699402022096&#45;&gt;132699402016976+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.01,-86.01C403.41,-83.96 412.75,-81.92 421.22,-80.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"422.04,-83.48 431.06,-77.93 420.55,-76.64 422.04,-83.48\"/>\n",
       "</g>\n",
       "<!-- 132699402021712 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>132699402021712</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"3,-0.5 3,-36.5 130,-36.5 130,-0.5 3,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"15.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"28,-0.5 28,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n",
       "</g>\n",
       "<!-- 132699402021712&#45;&gt;132699402011536* -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>132699402021712&#45;&gt;132699402011536*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.13,-31.76C140.38,-33.93 150.67,-36.11 159.93,-38.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159.4,-41.54 169.91,-40.19 160.85,-34.69 159.4,-41.54\"/>\n",
       "</g>\n",
       "<!-- 132699402011536 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>132699402011536</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"260.5,-27.5 260.5,-63.5 392.5,-63.5 392.5,-27.5 260.5,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"273\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"285.5,-27.5 285.5,-63.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n",
       "</g>\n",
       "<!-- 132699402011536&#45;&gt;132699402016976+ -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>132699402011536&#45;&gt;132699402016976+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M392.5,-59.16C402.35,-61.23 412.17,-63.29 421.04,-65.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"420.46,-68.61 430.96,-67.24 421.9,-61.76 420.46,-68.61\"/>\n",
       "</g>\n",
       "<!-- 132699402011536*&#45;&gt;132699402011536 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>132699402011536*&#45;&gt;132699402011536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223,-45.5C231.11,-45.5 240.5,-45.5 250.24,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.47,-49 260.47,-45.5 250.47,-42 250.47,-49\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x78b07cb800d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Lets also make our expression more complicated.\n# Think of this as a tiny neural network:\n#   a, b  -> inputs being combined by a weight-like multiplication\n#   c     -> a bias term being added\n#   f     -> another weight applied to the result\n#   L     -> the final output (think: loss)\n# This mirrors exactly what happens inside a neuron: inputs * weights + bias,\n# then passed through more operations to produce a scalar loss.\n\na = Value(2.0, label='a')\nb = Value(-3.0, label='b')\nc = Value(10.0, label='c')\n\n# d = a*b + c, instead of doing this directly, lets first create a variable for a*b so that d operates on atomic units\ne = a*b\ne.label = 'e'\n\nd = e + c\nd.label = 'd'\n\nf = Value(-2.0, label='f')\n\nL = f * d\nL.label = 'L'\n\ndraw_dot(L)"
  },
  {
   "cell_type": "markdown",
   "id": "16b6657b",
   "metadata": {},
   "source": "Now this effectively constitutes a **forward pass** \u2014 we take in inputs (`a`, `b`, `c`, `f`), feed them through the expression graph, and produce a single scalar output `L`.\n\nIn a real neural network, this is identical to what happens when you run data through the network: each layer transforms the activations through learned weights and biases until you arrive at a final output, and then a loss function turns that output into a single number measuring how wrong the network was. That number is `L`.\n\nThe computation graph we just drew captures *every* operation that contributed to `L`. This is the structure that makes backpropagation possible \u2014 we know exactly which path each input took to reach the output, so we can work backwards to figure out how much each input contributed to the final value."
  },
  {
   "cell_type": "markdown",
   "id": "3fdf312a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9537904",
   "metadata": {},
   "source": "## Section 2.2: Manual Backward Pass\n\nIn the forward pass we computed `L` from the inputs. Now we run the **backward pass**: computing the derivative of `L` with respect to every node in the graph \u2014 i.e. how much does `L` change if we nudge each intermediate value or input by a tiny amount.\n\nThis is backpropagation. The key mathematical tool is the **chain rule**: if `L` depends on `d`, and `d` depends on `a`, then:\n\n$$\n\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial d} \\cdot \\frac{\\partial d}{\\partial a}\n$$\n\nWe start at the output (`L`) where the gradient is trivially 1 (L changes at rate 1 with respect to itself), then propagate backwards through each operation, multiplying local gradients as we go. We'll do this manually first so the mechanics are completely clear before automating it."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e9d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complete_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}