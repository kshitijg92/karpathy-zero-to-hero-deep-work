{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be working through [Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I) by Andrej Karpathy. This is the third video in the \"Neural Networks: Zero to Hero\" series.\n",
    "\n",
    "Where the previous video built a **bigram** character-level language model (predicting the next character from only the _previous_ character), this video takes a massive leap: we implement the core ideas from Bengio et al. 2003 \u2014 [\"A Neural Probabilistic Language Model\"](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) \u2014 one of the most influential papers in NLP history.\n",
    "\n",
    "**The key idea:** Instead of conditioning on just one previous character, we use a **fixed-size context window** (e.g., the previous 3 characters) and feed them through a neural network (MLP) to predict the next character. This is a fundamentally different approach from counting bigrams:\n",
    "\n",
    "1. **Learned embeddings** \u2014 each character gets a dense vector representation (not a one-hot), and similar characters end up nearby in embedding space. This is the same idea behind `nn.Embedding` in PyTorch and the token embeddings in every modern transformer.\n",
    "2. **Shared statistical strength** \u2014 because the model operates on embeddings rather than raw character identities, it can generalize: if it learns that `da` is a common pattern, it can partially transfer that knowledge to `do` because `a` and `o` might be nearby in embedding space (both vowels).\n",
    "3. **Multi-character context** \u2014 by looking at 3 characters instead of 1, the model can learn much richer patterns about which character sequences are plausible in English names.\n",
    "\n",
    "**Connection to modern transformers:** The architecture we build here \u2014 embedding lookup \u2192 concatenation \u2192 hidden layer \u2192 output distribution \u2014 is essentially the \"embedding + MLP head\" that appears in every transformer. Transformers replace the fixed context window with self-attention (allowing variable-length context), but the embedding layer and the final prediction head work exactly the same way. Bengio 2003 laid the conceptual foundation that GPT, BERT, and every modern LLM builds on.\n",
    "\n",
    "We'll still be using our trusty `names.txt` dataset \u2014 32K names, character-level, predicting the next character autoregressively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dataset & Character Mappings\n",
    "\n",
    "We load the same `names.txt` dataset from lesson 2 and build the same character-to-integer mappings. Our vocabulary is 27 characters: the special token `.` (index 0) representing both start and end of a name, plus the 26 lowercase letters `a`-`z` (indices 1-26).\n",
    "\n",
    "These mappings (`stoi` and `itos`) are used throughout the notebook \u2014 every time we need to convert between human-readable characters and the integer indices that tensors require.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to read dataset\n",
    "DATASET_PATH = './names.txt'\n",
    "SPECIAL_TOKEN = \".\"\n",
    "\n",
    "def get_dataset():\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        rows = [row.strip() for row in f.readlines()]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 names loaded\n",
      "Examples: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "Vocabulary size: 27\n",
      "Mappings: {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "words = get_dataset()\n",
    "print(f\"{len(words)} names loaded\")\n",
    "print(f\"Examples: {words[:8]}\")\n",
    "\n",
    "# Build character mappings \u2014 identical to lesson 2\n",
    "# '.' is our special start/end token at index 0, then a=1, b=2, ..., z=26\n",
    "all_characters = [SPECIAL_TOKEN] + sorted(list(set(''.join(words))))\n",
    "stoi = {s: i for i, s in enumerate(all_characters)}\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Mappings: {itos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Building the Dataset\n",
    "\n",
    "This is where the MLP approach fundamentally differs from bigrams. Instead of looking at just one previous character, we use a **sliding window** of `block_size` characters as context to predict the next character.\n",
    "\n",
    "With `block_size=3`, the context window slides across each name like this. Take the name `emma`:\n",
    "\n",
    "| Context (X) | Target (Y) | Meaning                                    |\n",
    "| :---------: | :--------: | :----------------------------------------- |\n",
    "|    `...`    |    `e`     | Given nothing (start of name), predict `e` |\n",
    "|    `..e`    |    `m`     | Given `e` at the end, predict `m`          |\n",
    "|    `.em`    |    `m`     | Given `em`, predict `m`                    |\n",
    "|    `emm`    |    `a`     | Given `emm`, predict `a`                   |\n",
    "|    `mma`    |    `.`     | Given `mma`, predict end-of-name           |\n",
    "\n",
    "Each row becomes one training example: X is a tensor of 3 integers (the context character indices), and Y is a single integer (the target character index). A name of length L produces L+1 training examples (same as bigrams, but now each example carries more context).\n",
    "\n",
    "**Why pad with dots?** At the start of a name, we don't have 3 characters of history yet. We pad with `.` (index 0) to fill the context window. This is equivalent to the `<BOS>` (beginning of sequence) token in transformer models. The model learns that a context of `...` means \"start of a name\" and adjusts its predictions accordingly.\n",
    "\n",
    "**Terminology note:** Our `block_size` is what transformers call the **sequence length** (or **context length**). In GPT-2 the sequence length is 1024 tokens, in GPT-4 it's 128K tokens \u2014 but it's the same idea: a fixed-size window of previous tokens that the model conditions on when predicting the next one. The key difference is that our MLP processes all `block_size` tokens by concatenating their embeddings into a single flat vector, while transformers use self-attention to let each position attend to every other position in the sequence. This is why transformers scale to much longer contexts \u2014 attention is far more parameter-efficient than concatenation when the sequence length is large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: yuheng\n",
      "        Context --> Target\n",
      "------------------------------\n",
      "            ... --> y\n",
      "            ..y --> u\n",
      "            .yu --> h\n",
      "            yuh --> e\n",
      "            uhe --> n\n",
      "            hen --> g\n",
      "            eng --> .\n"
     ]
    }
   ],
   "source": [
    "# Let's see the sliding window in action for the name \"emma\"\n",
    "block_size = 3 # number of characters of context to use for prediction\n",
    "\n",
    "for w in words[:1]: # just \"emma\"\n",
    "    print(f\"Name: {w}\")\n",
    "    print(f\"{'Context':>15} --> Target\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Pad the beginning with block_size dots, append one dot for end-of-name\n",
    "    context = [0] * block_size\n",
    "    for ch in w + SPECIAL_TOKEN:\n",
    "        ix = stoi[ch]\n",
    "        context_str = ''.join(itos[i] for i in context)\n",
    "        print(f\"{context_str:>15} --> {ch}\")\n",
    "        context = context[1:] + [ix] # slide the window: drop oldest, append newest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: emma\n",
      "        Context --> Target\n",
      "------------------------------\n",
      "     .......... --> e\n",
      "     .........e --> m\n",
      "     ........em --> m\n",
      "     .......emm --> a\n",
      "     ......emma --> .\n"
     ]
    }
   ],
   "source": [
    "# Let's see the sliding window in action for the name \"emma\"\n",
    "block_size = 10 # What if we use a context window of length 10?\n",
    "\n",
    "for w in words[:1]: # just \"emma\"\n",
    "    print(f\"Name: {w}\")\n",
    "    print(f\"{'Context':>15} --> Target\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Pad the beginning with block_size dots, append one dot for end-of-name\n",
    "    context = [0] * block_size\n",
    "    for ch in w + SPECIAL_TOKEN:\n",
    "        ix = stoi[ch]\n",
    "        context_str = ''.join(itos[i] for i in context)\n",
    "        print(f\"{context_str:>15} --> {ch}\")\n",
    "        context = context[1:] + [ix] # slide the window: drop oldest, append newest",
    "\n",
    "# Reset block_size back to 3 for the rest of the notebook\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size=3):\n",
    "    \"\"\"\n",
    "    Convert a list of words into (X, Y) tensors for training.\n",
    "\n",
    "    X shape: (N, block_size) \u2014 each row is a context window of character indices\n",
    "    Y shape: (N,) \u2014 each element is the target character index\n",
    "\n",
    "    This function will be called three times: once each for train, val, and test splits.\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + SPECIAL_TOKEN:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([228146, 3])\n",
      "Y shape: torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# Build the full dataset to verify shapes and content\n",
    "X, Y = build_dataset(words, block_size)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 examples (from 'emma'):\n",
      "  ... --> e\n",
      "  ..e --> m\n",
      "  .em --> m\n",
      "  emm --> a\n",
      "  mma --> .\n"
     ]
    }
   ],
   "source": [
    "# Verify: print first 5 examples\n",
    "print(f\"\\nFirst 5 examples (from '{words[0]}'):\")\n",
    "for i in range(5):\n",
    "    context = ''.join(itos[ix.item()] for ix in X[i])\n",
    "    target = itos[Y[i].item()]\n",
    "    print(f\"  {context} --> {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Train/Val/Test Splits\n",
    "\n",
    "With the bigram model from lesson 2, we didn't bother with train/val/test splits because the model had very few effective parameters (just the 27\u00d727 count matrix). There wasn't much risk of overfitting.\n",
    "\n",
    "But our MLP will have thousands of learnable parameters, and with enough capacity, a neural network can simply **memorize** the training data rather than learning generalizable patterns. Train/val/test splits let us detect this:\n",
    "\n",
    "- **Training set (80%)** \u2014 the model learns from this data (gradient updates)\n",
    "- **Validation set (10%)** \u2014 we evaluate on this during training to tune hyperparameters (learning rate, embedding size, hidden layer size, etc.) and detect overfitting. If train loss keeps dropping but val loss plateaus or rises, we're overfitting.\n",
    "- **Test set (10%)** \u2014 touched only once at the very end, to report the model's true generalization performance. This prevents us from accidentally overfitting to the validation set through repeated hyperparameter tuning.\n",
    "\n",
    "**Important!:** We split at the _word_ level, not the _example_ level. If we split examples randomly, the model might see `emm \u2192 a` in training and `mma \u2192 .` in validation \u2014 both from the same name \"emma\". That would leak information and make our val loss unrealistically optimistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   182,580 examples from 25,626 names\n",
      "Validation:  22,767 examples from 3,203 names\n",
      "Test:        22,799 examples from 3,204 names\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# 80/10/10 split at the word level\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1], block_size=3)     # training\n",
    "Xdev, Ydev = build_dataset(words[n1:n2], block_size=3)  # validation (\"dev\" set)\n",
    "Xte, Yte = build_dataset(words[n2:], block_size=3)      # test\n",
    "\n",
    "print(f\"Training:   {Xtr.shape[0]:>7,} examples from {n1:,} names\")\n",
    "print(f\"Validation: {Xdev.shape[0]:>7,} examples from {n2-n1:,} names\")\n",
    "print(f\"Test:       {Xte.shape[0]:>7,} examples from {len(words)-n2:,} names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: The Embedding Layer\n",
    "\n",
    "This is Bengio's key innovation and arguably the most important idea in the paper. Instead of representing each character as a one-hot vector (a 27-dimensional vector with a single 1), we learn a **dense embedding** for each character \u2014 a short vector (say, 2 or 10 dimensions) that captures the character's \"meaning\" in a continuous space.\n",
    "\n",
    "**Why not one-hot?** A one-hot vector treats every pair of characters as equally different \u2014 `a` is as far from `b` as it is from `z`. But in names, some characters are more interchangeable than others: vowels can often substitute for each other (`dan` vs `den`), certain consonant clusters behave similarly. A learned embedding lets the model discover these relationships automatically: characters that behave similarly in context will end up with similar embedding vectors.\n",
    "\n",
    "**How it works:** We create a lookup table `C` of shape `(vocab_size, embed_dim)` \u2014 each row is a character's embedding vector. To embed a character, we simply index into this table: `C[5]` gives us the embedding for character 5 (`e`). This is mathematically identical to multiplying a one-hot vector by the embedding matrix (which we had seen in the previous notebook), but indexing is much faster.\n",
    "\n",
    "**Connection to PyTorch and transformers:** This is exactly what `nn.Embedding(vocab_size, embed_dim)` does under the hood. And in every transformer (GPT, BERT, LLaMA), the very first operation is an embedding lookup: each token ID gets mapped to a dense vector via a learned embedding table. The idea started here in Bengio 2003.\n",
    "\n",
    "We'll start with `embed_dim=2` so we can visualize the embeddings in 2D, then scale up later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding table shape: torch.Size([27, 2])\n",
      "Embedding for '.' (idx 0): tensor([ 1.5674, -0.2373])\n",
      "Embedding for 'a' (idx 1): tensor([-0.0274, -1.1008])\n",
      "Embedding for 'e' (idx 5): tensor([-0.4713,  0.7868])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 2 # embedding dimensionality \u2014 start small so we can visualize in 2D\n",
    "\n",
    "# The embedding table: each of the 27 characters gets a 2D vector\n",
    "# This is a learnable parameter \u2014 the model will adjust these vectors during training\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, embed_dim), generator=g)\n",
    "\n",
    "print(f\"Embedding table shape: {C.shape}\")\n",
    "print(f\"Embedding for '.' (idx 0): {C[0]}\")\n",
    "print(f\"Embedding for 'a' (idx 1): {C[1]}\")\n",
    "print(f\"Embedding for 'e' (idx 5): {C[5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[5] (embedding for 'e'): tensor([-0.4713,  0.7868])\n",
      "\n",
      "Context X[1] = [0, 0, 5] ('. . e')\n",
      "Embedded: C[X[1]] shape = torch.Size([3, 2])\n",
      "C[X[1]] = \n",
      "tensor([[ 1.5674, -0.2373],\n",
      "        [ 1.5674, -0.2373],\n",
      "        [-0.4713,  0.7868]])\n"
     ]
    }
   ],
   "source": [
    "# Embedding lookup via indexing \u2014 this is the core operation\n",
    "# When we index C with a tensor of integers, PyTorch returns the corresponding rows\n",
    "\n",
    "# Single character lookup:\n",
    "print(f\"C[5] (embedding for 'e'): {C[5]}\")\n",
    "\n",
    "# Batch lookup \u2014 index with a whole tensor at once:\n",
    "# If X[1] = [0, 0, 5] (context '..e'), then C[X[1]] gives us three 2D embeddings, one for each integer in X[1] tensor\n",
    "print(f\"\\nContext X[1] = {X[1].tolist()} ('{' '.join(itos[i.item()] for i in X[1])}')\")\n",
    "print(f\"Embedded: C[X[1]] shape = {C[X[1]].shape}\")\n",
    "print(f\"C[X[1]] = \\n{C[X[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr shape:      torch.Size([182580, 3])\n",
      "Embedded shape: torch.Size([182580, 3, 2])  \u2014 (num_examples, block_size, embed_dim)\n"
     ]
    }
   ],
   "source": [
    "# Embed the entire training set at once \u2014 PyTorch handles arbitrary index tensor shapes\n",
    "# Xtr has shape (N, 3), so C[Xtr] has shape (N, 3, 2): each of the 3 context characters\n",
    "# gets replaced by its 2D embedding vector\n",
    "emb = C[Xtr]\n",
    "print(f\"Xtr shape:      {Xtr.shape}\")\n",
    "print(f\"Embedded shape: {emb.shape}  \u2014 (num_examples, block_size, embed_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Embeddings: `view()` vs `reshape()` vs `flatten()`\n",
    "\n",
    "After embedding, we have a tensor of shape `(N, 3, 2)` \u2014 N examples, each with 3 context characters, each represented by a 2D embedding. But our hidden layer expects a single flat vector per example, so we need to go from `(N, 3, 2)` \u2192 `(N, 6)`. There are several ways to do this in PyTorch:\n",
    "\n",
    "1. **`torch.cat`** \u2014 manually concatenate along a dimension: `torch.cat([emb[:, 0], emb[:, 1], emb[:, 2]], dim=1)`. Correct but verbose, and creates a new tensor by copying data.\n",
    "\n",
    "2. **`tensor.reshape(N, 6)`** \u2014 reshapes to the target shape. Always works, but may or may not copy data depending on whether the tensor is contiguous in memory.\n",
    "\n",
    "3. **`tensor.view(N, 6)`** \u2014 reshapes to the target shape **without ever copying data**. It just reinterprets the same underlying memory with different stride metadata. This means it's essentially free (O(1) \u2014 no data movement), but it will raise an error if the tensor's memory layout isn't compatible (i.e., the tensor isn't contiguous).\n",
    "\n",
    "4. **`tensor.flatten(start_dim=1)`** \u2014 syntactic sugar for `.view(N, -1)` or `.reshape(N, -1)`. Convenient and readable, but under the hood it's just calling reshape.\n",
    "\n",
    "**Why we use `.view()` here:** Since `C[Xtr]` produces a contiguous tensor (embedding lookup always returns contiguous memory), `.view()` is guaranteed to work and is the most efficient option \u2014 it's a zero-cost reshape that shares memory with the original tensor. Any changes to the viewed tensor affect the original and vice versa, which is exactly what we want for backpropagation (gradients flow through views seamlessly).\n",
    "\n",
    "**When would `.view()` fail?** If you transpose or permute a tensor, it becomes non-contiguous (the elements are no longer laid out sequentially in memory). In that case, `.view()` raises a `RuntimeError` and you'd need `.reshape()` (which internally calls `.contiguous().view()`) or `.contiguous().view()` explicitly. But for our embedding concatenation, this never happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated shape: torch.Size([182580, 6])  \u2014 (num_examples, block_size * embed_dim)\n",
      "\n",
      "First example: tensor([ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373])\n",
      "Same as manual concat: tensor([ 1.5674, -0.2373,  1.5674, -0.2373,  1.5674, -0.2373])\n"
     ]
    }
   ],
   "source": [
    "# To feed into the hidden layer, we need to concatenate the embeddings for all context\n",
    "# characters into a single flat vector. Shape goes from (N, 3, 2) to (N, 6).\n",
    "#\n",
    "# We use .view() which reshapes without copying data (it just changes the stride metadata).\n",
    "# This is equivalent to .reshape() but guarantees the result shares memory with the original.\n",
    "#\n",
    "# The -1 in .view(emb.shape[0], -1) tells PyTorch to infer the size: N * block_size * embed_dim / N = 6\n",
    "emb_cat = emb.view(emb.shape[0], -1)\n",
    "print(f\"Concatenated shape: {emb_cat.shape}  \u2014 (num_examples, block_size * embed_dim)\")\n",
    "print(f\"\\nFirst example: {emb_cat[0]}\")\n",
    "print(f\"Same as manual concat: {torch.cat([emb[0, 0], emb[0, 1], emb[0, 2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: MLP Architecture\n",
    "\n",
    "Now we put it all together. The architecture follows Bengio 2003 closely:\n",
    "\n",
    "```\n",
    "Input: 3 character indices\n",
    "  \u2502\n",
    "  \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Embedding Lookup (C)       \u2502  Shape: (N, 3) \u2192 (N, 3, embed_dim)\n",
    "\u2502  Each index \u2192 dense vector  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "  \u2502\n",
    "  \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Concatenate                \u2502  Shape: (N, 3, embed_dim) \u2192 (N, 3 * embed_dim)\n",
    "\u2502  Flatten context embeddings \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "  \u2502\n",
    "  \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Hidden Layer (W1, b1)      \u2502  Shape: (N, 3*embed_dim) \u2192 (N, n_hidden)\n",
    "\u2502  Linear + tanh activation   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "  \u2502\n",
    "  \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Output Layer (W2, b2)      \u2502  Shape: (N, n_hidden) \u2192 (N, vocab_size)\n",
    "\u2502  Linear (logits)            \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "  \u2502\n",
    "  \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Softmax + Cross-Entropy    \u2502  Shape: (N, 27) \u2192 scalar loss\n",
    "\u2502  (via F.cross_entropy)      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Why `tanh`?** We need a nonlinearity between the hidden and output layers \u2014 without one, stacking two linear layers is equivalent to a single linear layer (matrix multiplication is associative). `tanh` squashes values to [-1, 1] and was the standard choice in 2003. Modern networks typically use ReLU or GELU, but `tanh` works well for this small model and is what Karpathy uses in the video.\n",
    "\n",
    "**Why `F.cross_entropy` instead of manual softmax + NLL?** `F.cross_entropy` combines log-softmax and negative log-likelihood in a single, numerically stable operation. Doing softmax manually can lead to overflow (large logits \u2192 exp overflow) or underflow (taking log of tiny probabilities). PyTorch's implementation uses the log-sum-exp trick to avoid these issues. This is the standard loss function for classification problems and what every language model uses. **NOTE:** We already cross checked that our manual NLL calculation is the same as `F.cross_entropy` in the previous lesson notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6,881\n",
      "     C:      torch.Size([27, 2]) =     54 params\n",
      "    W1:     torch.Size([6, 200]) =  1,200 params\n",
      "    b1:        torch.Size([200]) =    200 params\n",
      "    W2:    torch.Size([200, 27]) =  5,400 params\n",
      "    b2:         torch.Size([27]) =     27 params\n"
     ]
    }
   ],
   "source": [
    "# Initialize all parameters with a fixed seed for reproducibility\n",
    "embed_dim = 10    # embedding dimensionality \u2014 scaled up from 2D\n",
    "n_hidden = 200 # hidden layer size\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C  = torch.randn((vocab_size, embed_dim),            generator=g)\n",
    "W1 = torch.randn((embed_dim * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden,                        generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size),           generator=g)\n",
    "b2 = torch.randn(vocab_size,                       generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "# Total parameter count\n",
    "total_params = sum(p.nelement() for p in parameters)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "for name, p in zip(['C', 'W1', 'b1', 'W2', 'b2'], parameters):\n",
    "    print(f\"  {name:>4}: {str(p.shape):>24} = {p.nelement():>6,} params\")\n",
    "\n",
    "# Enable gradient tracking for all parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Embedding:     torch.Size([182580, 3, 2])\n",
      "2. Concatenated:  torch.Size([182580, 6])\n",
      "3. Hidden (tanh): torch.Size([182580, 200])\n",
      "4. Logits:        torch.Size([182580, 27])\n",
      "5. Loss:          21.9245\n",
      "   Ideal initial (uniform): 3.2958\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step forward pass walkthrough (on the full training set, just to see shapes)\n",
    "# In training we'll use mini-batches instead\n",
    "\n",
    "# Step 1: Embed the context characters\n",
    "emb = C[Xtr] # (N, block_size, embed_dim)\n",
    "print(f\"1. Embedding:     {emb.shape}\")\n",
    "\n",
    "# Step 2: Concatenate embeddings into a single vector per example\n",
    "emb_cat = emb.view(emb.shape[0], -1) # (N, block_size * embed_dim)\n",
    "print(f\"2. Concatenated:  {emb_cat.shape}\")\n",
    "\n",
    "# Step 3: Hidden layer \u2014 linear transform + tanh activation\n",
    "h = torch.tanh(emb_cat @ W1 + b1) # (N, n_hidden)\n",
    "print(f\"3. Hidden (tanh): {h.shape}\")\n",
    "\n",
    "# Step 4: Output layer \u2014 linear transform to get logits (unnormalized log-probabilities)\n",
    "logits = h @ W2 + b2 # (N, vocab_size)\n",
    "print(f\"4. Logits:        {logits.shape}\")\n",
    "\n",
    "# Step 5: Loss \u2014 cross-entropy between predicted distribution and true next character\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "print(f\"5. Loss:          {loss.item():.4f}\")\n",
    "# print(\"\\n\")\n",
    "print(f\"   Ideal initial (uniform): {-torch.tensor(1/27.).log().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Is the Initial Loss So High?\n",
    "\n",
    "With 27 possible characters, a model that predicts a perfectly **uniform distribution** (equal probability 1/27 for every character) would have a loss of `-ln(1/27) \u2248 3.30`. That's the baseline \u2014 the loss of a model that knows nothing but at least isn't confidently wrong.\n",
    "\n",
    "Our initial loss is **much** higher than 3.30. This is because `torch.randn` initializes the weights with standard normal values (mean 0, std 1), which produces **large logits** in the output layer. Large logits mean the softmax is sharply peaked \u2014 the model is very *confident* in its predictions, but since the weights are random, it's confidently predicting the *wrong* character almost every time. Being confidently wrong is far worse than being uniformly uncertain.\n",
    "\n",
    "**The math:** If the model assigns probability 0.95 to the wrong character and 0.01 to the correct one, the loss for that example is `-ln(0.01) = 4.6`. Average that over many examples with similarly miscalibrated predictions and the loss balloons well above 3.30.\n",
    "\n",
    "**The fix (Part 3 preview):** If we initialized the weights so that the initial logits were all close to zero, the softmax would output a nearly uniform distribution and our initial loss would start right at ~3.30. This is the idea behind **Kaiming initialization** \u2014 scale the weights so that activations neither explode nor vanish as they pass through each layer. We'll implement this properly in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Training\n",
    "\n",
    "Now we train the model using **mini-batch stochastic gradient descent (SGD)**.\n",
    "\n",
    "**Why mini-batches instead of full-batch?** With ~180K training examples, computing the forward and backward pass on the entire dataset every step would be slow. Instead, we randomly sample a small batch (32 examples) each step. This has two benefits:\n",
    "\n",
    "1. **Speed** \u2014 each step is much faster, so we can do many more steps in the same wall-clock time\n",
    "2. **Noise as regularization** \u2014 the randomness in mini-batch sampling adds noise to the gradient estimates, which actually helps the model generalize better (it's harder to memorize when you're being pushed in slightly different directions each step)\n",
    "\n",
    "The trade-off is that each individual gradient estimate is noisier (based on only 32 examples instead of 180K), but empirically this works very well \u2014 virtually all modern deep learning uses mini-batch SGD (or variants like Adam).\n",
    "\n",
    "**Learning rate schedule:** We start with a learning rate of 0.1 (aggressive, to make fast initial progress) and decay to 0.01 after 100K steps (to fine-tune and converge to a lower loss). Finding the right learning rate is one of the most important hyperparameters \u2014 too high and the loss will diverge, too low and training will be painfully slow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# First, let's find a good learning rate using the LR sweep technique\n# Idea: exponentially increase LR from very small to very large over many steps,\n# record the loss at each step, and plot loss vs LR.\n# The optimal LR is in the region where the loss is decreasing most steeply.\n\n# Re-initialize parameters for a clean sweep\ng = torch.Generator().manual_seed(2147483647)\nC  = torch.randn((vocab_size, embed_dim),            generator=g)\nW1 = torch.randn((embed_dim * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),           generator=g)\nb2 = torch.randn(vocab_size,                       generator=g)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n    p.requires_grad = True\n\n# LR sweep: 1000 steps, LR goes from 0.001 to 1.0 exponentially\nlre = torch.linspace(-3, 0, 1000) # exponents: -3 to 0\nlrs = 10 ** lre                    # actual LRs: 0.001 to 1.0\n\nlri = [] # track which LR was used\nlossi = [] # track the loss\n\nfor i in range(1000):\n    # Mini-batch\n    ixds = torch.randint(0, Xtr.shape[0], (32,))\n    mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n\n    # Forward pass\n    emb = C[mini_batch_inp]\n    h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)\n    logits = h @ W2 + b2\n    loss = F.cross_entropy(logits, mini_batch_target)\n\n    # Backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # Update with current LR\n    lr = lrs[i].item()\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    lri.append(lr)\n    lossi.append(loss.log10().item())\n\nprint(f\"Sweep complete. Final loss: {10**lossi[-1]:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs learning rate \u2014 look for the \"sweet spot\" where loss drops fastest\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(lri, lossi)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss (log10)')\n",
    "plt.title('Learning Rate Finder')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# The optimal region is typically where the curve is steepest (before it starts going back up).\n",
    "# For this model, that's usually around LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now train for real with the learning rate we found\n\n# Re-initialize parameters for a clean training run\ng = torch.Generator().manual_seed(2147483647)\nC  = torch.randn((vocab_size, embed_dim),            generator=g)\nW1 = torch.randn((embed_dim * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),           generator=g)\nb2 = torch.randn(vocab_size,                       generator=g)\nparameters = [C, W1, b1, W2, b2]\nfor p in parameters:\n    p.requires_grad = True\n\nlossi = []\nstepi = []\n\nfor i in range(200000):\n    # Mini-batch: randomly sample 32 examples\n    ixds = torch.randint(0, Xtr.shape[0], (32,))\n    mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n\n    # Forward pass\n    emb = C[mini_batch_inp]                                        # (32, 3, 10)\n    h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)  # (32, 200)\n    logits = h @ W2 + b2                                           # (32, 27)\n    loss = F.cross_entropy(logits, mini_batch_target)\n\n    # Backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # Learning rate step decay: 0.1 for first 100K steps, then 0.01\n    lr = 0.1 if i < 100000 else 0.01\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # Track loss\n    stepi.append(i)\n    lossi.append(loss.log10().item())\n\nprint(f\"Final training loss (last batch): {loss.item():.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over time\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(stepi, lossi, alpha=0.3)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (log10)')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# You can see the loss drop quickly in the first few thousand steps,\n",
    "# then gradually improve. The jump at step 100K is when we reduce the LR from 0.1 to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on both training and validation sets\n",
    "# Use the full dataset (not mini-batches) for an accurate loss estimate\n",
    "\n",
    "@torch.no_grad() # disable gradient tracking for evaluation \u2014 saves memory and compute\n",
    "def eval_loss(X, Y):\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()\n",
    "\n",
    "train_loss = eval_loss(Xtr, Ytr)\n",
    "val_loss = eval_loss(Xdev, Ydev)\n",
    "print(f\"Training loss:   {train_loss:.4f}\")\n",
    "print(f\"Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "# For comparison, the bigram model from lesson 2 achieved ~2.45 loss.\n",
    "# Our MLP should do noticeably better (~2.3 or lower) thanks to the longer context window.\n",
    "# If train loss << val loss, we're overfitting and should reduce model capacity or add regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Visualizing the Embeddings\n",
    "\n",
    "One of the most beautiful aspects of learned embeddings is that they organize themselves meaningfully without any explicit supervision. The model was never told that `a`, `e`, `i`, `o`, `u` are vowels \u2014 it discovered this implicitly from the data, because vowels tend to appear in similar contexts (between consonants, often interchangeable).\n",
    "\n",
    "To visualize this, we'll train a model with `embed_dim=2` so each character's embedding is just a 2D point we can plot on a scatter plot. With higher-dimensional embeddings, you'd need dimensionality reduction (PCA, t-SNE) to visualize, but 2D lets us see the raw learned space directly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train a 2D embedding model specifically for visualization\nembed_dim_viz = 2\nn_hidden_viz = 200\n\ng = torch.Generator().manual_seed(2147483647)\nC_viz  = torch.randn((vocab_size, embed_dim_viz),                generator=g)\nW1_viz = torch.randn((embed_dim_viz * block_size, n_hidden_viz), generator=g)\nb1_viz = torch.randn(n_hidden_viz,                            generator=g)\nW2_viz = torch.randn((n_hidden_viz, vocab_size),              generator=g)\nb2_viz = torch.randn(vocab_size,                              generator=g)\nparameters_viz = [C_viz, W1_viz, b1_viz, W2_viz, b2_viz]\nfor p in parameters_viz:\n    p.requires_grad = True\n\n# Train for 200K steps (same schedule)\nfor i in range(200000):\n    ixds = torch.randint(0, Xtr.shape[0], (32,))\n    mini_batch_inp, mini_batch_target = Xtr[ixds], Ytr[ixds]\n\n    emb = C_viz[mini_batch_inp]\n    h = torch.tanh(emb.view(-1, embed_dim_viz * block_size) @ W1_viz + b1_viz)\n    logits = h @ W2_viz + b2_viz\n    loss = F.cross_entropy(logits, mini_batch_target)\n    for p in parameters_viz:\n        p.grad = None\n    loss.backward()\n    lr = 0.1 if i < 100000 else 0.01\n    for p in parameters_viz:\n        p.data += -lr * p.grad\n\nprint(f\"2D model final loss: {loss.item():.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the 2D character embeddings\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(C_viz[:, 0].data, C_viz[:, 1].data, s=200, c='lightblue', edgecolors='black')\n",
    "for i in range(vocab_size):\n",
    "    plt.annotate(itos[i], (C_viz[i, 0].item(), C_viz[i, 1].item()),\n",
    "                 ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "plt.title('Learned Character Embeddings (2D)', fontsize=14)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Look for structure:\n",
    "# - Vowels (a, e, i, o, u) tend to cluster together\n",
    "# - The special token '.' is usually off on its own (it behaves very differently from letters)\n",
    "# - Characters that appear in similar contexts (e.g., similar consonants) may be nearby\n",
    "# The exact layout changes with different random seeds, but the clustering is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Sampling from the Model\n",
    "\n",
    "Sampling from our MLP works the same way as the bigram model \u2014 **autoregressive generation**. We start with a context of all dots (`...`), predict a probability distribution over the next character, sample from it, shift the context window, and repeat until we sample a dot (end-of-name).\n",
    "\n",
    "The key difference from bigrams: because we're using 3 characters of context instead of 1, the generated names should sound much more plausible. The model can learn patterns like \"consonant clusters that are common at the start of names\" or \"vowel patterns that are typical in the middle\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names from the trained model (the 10D model, not the 2D visualization model)\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "print(\"Generated names:\")\n",
    "print(\"-\" * 20)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # start with '...'\n",
    "    while True:\n",
    "        # Forward pass on the current context\n",
    "        emb = C[torch.tensor([context])] # (1, block_size, embed_dim)\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Sample next character from the predicted distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "\n",
    "        # Shift context window and append the sampled character\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        if ix == 0: # sampled the end token\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Hyperparameter Exploration\n",
    "\n",
    "The model we've built has several hyperparameters that affect its performance. Understanding their trade-offs is a key part of ML engineering:\n",
    "\n",
    "| Hyperparameter | Our Value | Effect of Increasing |\n",
    "|:--------------|:---------:|:--------------------|\n",
    "| `embed_dim` | 10 | Richer character representations, but more parameters in C and W1 |\n",
    "| `n_hidden` | 200 | More expressive hidden layer, but more parameters and risk of overfitting |\n",
    "| `block_size` | 3 | More context for prediction, but larger W1 and more data needed |\n",
    "| Learning rate | 0.1\u21920.01 | Faster learning vs. stability; too high \u2192 divergence, too low \u2192 slow |\n",
    "| Batch size | 32 | Larger \u2192 more stable gradients but slower per-step; smaller \u2192 more noise |\n",
    "| Training steps | 200K | More steps \u2192 better convergence, but diminishing returns |\n",
    "\n",
    "**The overfitting sweet spot:** With ~11K parameters and ~180K training examples, our model is relatively safe from severe overfitting (roughly 16 examples per parameter). But if we made the model much larger (e.g., `n_hidden=1000`), we'd need to watch the train/val gap carefully.\n",
    "\n",
    "**What's next:** In Part 3, Karpathy addresses two critical issues we've glossed over here:\n",
    "1. **Initialization** \u2014 our random init can cause the tanh to saturate and loss to start very high. Proper initialization (like Kaiming/He init) fixes this.\n",
    "2. **Batch normalization** \u2014 a technique that stabilizes training by normalizing activations within each layer, making the model less sensitive to initialization and learning rate choices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Quick experiment cell \u2014 adjust hyperparameters and re-train to see the effect\n# Try changing these values and running this cell + the next one\n\ndef train_model(embed_dim=10, n_hidden=200, block_size=3, lr=0.1, lr_decay=0.01,\n                decay_at=100000, steps=200000, batch_size=32, seed=2147483647):\n    \"\"\"Train an MLP with the given hyperparameters and return train/val loss.\"\"\"\n    # Rebuild datasets with the given block_size\n    Xtr_, Ytr_ = build_dataset(words[:n1], block_size)\n    Xdev_, Ydev_ = build_dataset(words[n1:n2], block_size)\n\n    g = torch.Generator().manual_seed(seed)\n    C  = torch.randn((vocab_size, embed_dim),            generator=g)\n    W1 = torch.randn((embed_dim * block_size, n_hidden), generator=g)\n    b1 = torch.randn(n_hidden,                        generator=g)\n    W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n    b2 = torch.randn(vocab_size,                      generator=g)\n    params = [C, W1, b1, W2, b2]\n    for p in params:\n        p.requires_grad = True\n\n    for i in range(steps):\n        ixds = torch.randint(0, Xtr_.shape[0], (batch_size,))\n        mini_batch_inp, mini_batch_target = Xtr_[ixds], Ytr_[ixds]\n\n        emb = C[mini_batch_inp]\n        h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)\n        logits = h @ W2 + b2\n        loss = F.cross_entropy(logits, mini_batch_target)\n        for p in params:\n            p.grad = None\n        loss.backward()\n        cur_lr = lr if i < decay_at else lr_decay\n        for p in params:\n            p.data += -cur_lr * p.grad\n\n    # Evaluate\n    with torch.no_grad():\n        emb = C[Xtr_]\n        h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)\n        logits = h @ W2 + b2\n        train_loss = F.cross_entropy(logits, Ytr_).item()\n\n        emb = C[Xdev_]\n        h = torch.tanh(emb.view(-1, embed_dim * block_size) @ W1 + b1)\n        logits = h @ W2 + b2\n        val_loss = F.cross_entropy(logits, Ydev_).item()\n\n    total_params = sum(p.nelement() for p in params)\n    print(f\"embed_dim={embed_dim}, n_hidden={n_hidden}, block_size={block_size}, \"\n          f\"params={total_params:,}, train={train_loss:.4f}, val={val_loss:.4f}\")\n    return train_loss, val_loss\n\n# Example: compare different embedding sizes\nprint(\"Comparing embedding sizes (this will take a minute)...\")\nfor ne in [2, 5, 10]:\n    train_model(embed_dim=ne, steps=50000)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we implemented the core ideas from Bengio et al. 2003, building a character-level MLP that significantly outperforms the bigram model from lesson 2:\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "- A **learned embedding table** that maps each character to a dense vector \u2014 the same concept behind `nn.Embedding` and transformer token embeddings\n",
    "- A **fixed-context MLP** that takes 3 characters of context, embeds and concatenates them, passes through a hidden layer with tanh activation, and outputs a probability distribution over the next character\n",
    "- **Mini-batch SGD training** with learning rate decay\n",
    "\n",
    "**Key results:**\n",
    "\n",
    "- Loss improved from ~2.45 (bigram) to ~2.3 (MLP) \u2014 a meaningful improvement from using more context\n",
    "- The learned 2D embeddings show that the model discovers structure (vowel/consonant clustering) without supervision\n",
    "- Generated names sound noticeably more plausible than bigram output\n",
    "\n",
    "**Key concepts introduced:**\n",
    "\n",
    "- Learned dense embeddings vs. one-hot encoding\n",
    "- Train/validation/test splits and overfitting detection\n",
    "- Mini-batch SGD and learning rate schedules\n",
    "- The learning rate finder technique\n",
    "- `F.cross_entropy` and why it's better than manual softmax + NLL\n",
    "\n",
    "**Looking ahead to Part 3 (Activations & Gradients):** The model we built here works, but we were somewhat lucky with our initialization. In the next video, Karpathy will show that random initialization can cause serious problems:\n",
    "\n",
    "- **tanh saturation** \u2014 if initial activations are too large, tanh outputs are all \u00b11 and gradients vanish\n",
    "- **Dead neurons** \u2014 some hidden units may never activate usefully\n",
    "\n",
    "The fix involves proper **weight initialization** (Kaiming/He init) and **batch normalization** \u2014 techniques that make training deep networks much more reliable. These are the same techniques used in production-scale models today.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complete_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}